{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import * \n",
    "\n",
    "############## ONLY NEED TO CHANGE THE FOLLOWING TO USE preprocess module##########\n",
    "DIR = 'fb_data' \n",
    "BOOK_FILE = '{}/cbt.txt'.format(DIR)\n",
    "BOOK_LEMMA_FILE = '{}/cbt_lemma.txt'.format(DIR)\n",
    "PPVT_LEMMA_FILE = '{}/PPVT_lemma.csv'.format(DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n",
      "43439\n",
      "228\n"
     ]
    }
   ],
   "source": [
    "ranked_words= read_rebecca_lemma(PPVT_LEMMA_FILE)\n",
    "book_words, pos_words = lemmatize_book(BOOK_FILE, BOOK_LEMMA_FILE)\n",
    "generate_wordset_files(book_words, ranked_words, DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "    - Good to see increase in distance for these 3 pairs of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n",
    "from gensim.models import KeyedVectors\n",
    "from igraph import *\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\"\"\"\n",
    "task\n",
    "1. `model.py`: construct simple pipeline to build igraph, form df\n",
    "2. `preprocess.py`: build several graphs (V,N) in ipython book\n",
    "3. `visualize.py`: visualize network by sampling\n",
    "\"\"\"\n",
    "\n",
    "DIR = 'fb_data'\n",
    "# paths to the 3 generated files from preprocess.py\n",
    "BOOK_WORD_SET_FILE = '{}/book_words_set.txt'.format(DIR)\n",
    "REBECCA_WORD_FILE = '{}/rebecca_words.txt'.format(DIR)\n",
    "LEMMA_BOOK_FILE = \"{}/cbt_lemma.txt\".format(DIR)\n",
    "\n",
    "def read_file_to_list(filename):\n",
    "\twords = []\n",
    "\twith open(filename, 'r') as f:\n",
    "\t    for word in f:\n",
    "\t        words.append(word.strip())\n",
    "\treturn words\n",
    "\n",
    "def getDistance(w1, w2, model):\n",
    "\n",
    "    return abs(1-spatial.distance.cosine(model.wv[w1], model.wv[w2]))\n",
    "\n",
    "def load_pretrained_word2vec(documents, size, min_count, iters, retrain=False):\n",
    "\tGOOGLE_W2V_FILE = './model/GoogleNews-vectors-negative300.bin'\n",
    "\tmodel = KeyedVectors.load_word2vec_format(GOOGLE_W2V_FILE, binary=True)  \n",
    "\tif not retrain:\n",
    "\t\treturn model\n",
    "\tmodel_2 = gensim.models.Word2Vec(\n",
    "\t            documents,\n",
    "\t            sg=0,\n",
    "\t            size=size,\n",
    "\t            window=20,\n",
    "\t            min_count=min_count,\n",
    "\t            iter=iters,\n",
    "\t            workers=4)\n",
    "\ttotal_examples = model_2.corpus_count\n",
    "\tmodel_2.build_vocab([list(model.vocab.keys())], update=True)\n",
    "\tmodel_2.intersect_word2vec_format(GOOGLE_W2V_FILE, binary=True, lockf=1.0)\n",
    "\tmodel_2.train(documents, total_examples=total_examples, epochs=model_2.epochs)\n",
    "\treturn model_2\n",
    "\n",
    "def train_word2vec(documents, size=300, min_count=5, iters=100, window=20):\n",
    "\tmodel = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        sg=0,\n",
    "        size=size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        iter=iters,\n",
    "        workers=4)\n",
    "\tmodel.train(documents, total_examples=len(documents), epochs=model.epochs)\n",
    "\treturn model\n",
    "\n",
    "def build_graph(model, sampling_size=0):\n",
    "\n",
    "\tbook_set = read_file_to_list(BOOK_WORD_SET_FILE)\n",
    "\tedu_set = read_file_to_list(REBECCA_WORD_FILE)\n",
    "\n",
    "\tprint(\"word2vec:\\t\", len(model.wv.vocab))\n",
    "\tprint(\"book_set:\\t\", len(book_set))\n",
    "\tprint(\"edu_set:\\t\", len(edu_set))\n",
    "# \tsurrounding_words = set(model.wv.vocab.keys()).intersection(book_set)\n",
    "# \tsurrounding_words = set(surrounding_words).difference(edu_set)\n",
    "# \tprint('surrounding words', len(surrounding_words))\n",
    "\tintersection_words = set(model.wv.vocab.keys()).intersection(edu_set)\n",
    "\tprint('intersertion words', len(intersection_words))\n",
    "\n",
    "\tif sampling_size:\n",
    "\t\ttoken2vec = {}\n",
    "\t\trand_words = random.sample(intersection_words, sampling_size)\n",
    "\t\tfor word in rand_words:\n",
    "\t\t\ttoken2vec[word] = model.wv[word]\n",
    "\t\tfor word in intersection_words:\n",
    "\t\t\ttoken2vec[word] = model.wv[word]\n",
    "\telse:\n",
    "\t\ttoken2vec = model.wv.vocab # bad\n",
    "\n",
    "\tidx = 0\n",
    "\tid2token = {}\n",
    "\ttoken2id = {}\n",
    "\n",
    "\tfor word in token2vec:\n",
    "\t    id2token[idx] = word\n",
    "\t    token2id[word] = idx\n",
    "\t    idx += 1\n",
    "\n",
    "\tvertices = [idx for idx in range(len(token2vec))]\n",
    "\tedges = [(i, j) for i in vertices for j in vertices if i < j]\n",
    "\tg = Graph(vertex_attrs={\"label\":vertices}, edges=edges, directed=False)\n",
    "\tg.es[\"weight\"] = [getDistance(id2token[i], id2token[j], model) for i,j in edges]\n",
    "\n",
    "\t# test validity\n",
    "\t# assert(getDistance('man', 'woman', model) == g[token2id['man'], token2id['woman']] )\n",
    "\t# assert(getDistance('cat', 'dog', model) == g[token2id['cat'], token2id['dog']] )\n",
    "\n",
    "\treturn g, (edges, vertices), (id2token, token2id)\n",
    "\n",
    "def filter_graph(weights, edges, vertices, id2token, max_dist=1.0):\n",
    "\n",
    "\tnew_edges, new_weights = [], []\n",
    "\tfor edge, dist in zip(edges, weights):\n",
    "\t    if dist <= max_dist:\n",
    "\t        new_edges.append(edge)\n",
    "\t        new_weights.append(dist)\n",
    "\tg = Graph(vertex_attrs={\"label\":vertices}, edges=new_edges, directed=False)\n",
    "\n",
    "\tg.es[\"weight\"] = new_weights\n",
    "\tg.vs[\"label\"] = [id2token[idx] for idx in vertices]\n",
    "\n",
    "\treturn g\n",
    "\n",
    "def compute_measures_df(g, edges, vertices, id2token, token2id):\n",
    "\t\"\"\"\n",
    "\tcompute centrality measures to output df\n",
    "\t\"\"\"\n",
    "\n",
    "\t# g -> strength, closeness (continuous)\n",
    "\tstrengthRank = g.strength(None,  weights=g.es['weight'])\n",
    "\tclosenessRank = g.closeness(None, 'all', weights=g.es['weight'], normalized=True)\n",
    "\n",
    "\t# g1 -> betweenness, eigen_centrality\n",
    "\tg1 = filter_graph(g.es[\"weight\"], edges, vertices, id2token, max_dist=0.5)\n",
    "\n",
    "\tbetweennessRank = g1.betweenness(directed=False, weights=g1.es['weight'])\n",
    "\teigen_centralityRank = g1.eigenvector_centrality(directed=False, weights=g1.es['weight'])\n",
    "\n",
    "\t# g2 -> degree\n",
    "\tg2 = filter_graph(g.es[\"weight\"], edges, vertices, id2token, max_dist=0.1)\n",
    "\n",
    "\tdegreeRank = g2.degree(mode='all')\n",
    "\n",
    "\t# frequency\n",
    "\twordCounter = defaultdict(int)\n",
    "\twith open(LEMMA_BOOK_FILE) as f:\n",
    "\t    for line in f:\n",
    "\t        for w in line.split():\n",
    "\t            w = w.strip()\n",
    "\t            if w in token2id:\n",
    "\t                wordCounter[w] += 1\n",
    "\tfreqRank = [wordCounter[w] for w in token2id]\n",
    "\n",
    "\t# rebecca\n",
    "\twith open(REBECCA_WORD_FILE, 'r') as f:\n",
    "\t    edu_list = []\n",
    "\t    for word in f:\n",
    "\t        edu_list.append(word.strip())\n",
    "\t        \n",
    "\trebeccaRank = {w:i+1 for i, w in enumerate(edu_list)}\n",
    "\tfinal_words_set = set(token2id.keys()).intersection(edu_list)\n",
    "\n",
    "\t# create df\n",
    "\tdata = []\n",
    "\twords_inorder = [id2token[idx] for idx in range(len(token2id))]\n",
    "\tfor i, word in enumerate(words_inorder):\n",
    "\t    if word in final_words_set:\n",
    "\t        data.append([word,\n",
    "\t                    rebeccaRank[word],\n",
    "\t                    strengthRank[i], \n",
    "\t                    closenessRank[i], \n",
    "\t                    betweennessRank[i], \n",
    "\t                    eigen_centralityRank[i],\n",
    "\t                    degreeRank[i],\n",
    "\t                    freqRank[i]])\n",
    "\n",
    "\t\n",
    "\tdf = pd.DataFrame(data, columns=['word', 'ppvt', 'strgth', 'close', 'betw', 'eigen', 'degree','freq'])\n",
    "\tdf = df.sort_values(by=['ppvt'])\n",
    "\tprint(df.head())\n",
    "\treturn df\n",
    "\n",
    "def build_pos_graph(model, pos_word_set, sampling_size=0, max_dist=1):\n",
    "\t\"\"\"\n",
    "\tbuild semantic graph without referring Rebecca's word list;\n",
    "\tonly to explore noun, verb's centrality in vocab\n",
    "\t\"\"\"\n",
    "\n",
    "\tbook_set = read_file_to_list(BOOK_WORD_SET_FILE)\n",
    "\tedu_set = read_file_to_list(REBECCA_WORD_FILE)\n",
    "\tvalid_pos_words = set(model.wv.vocab.keys()).intersection(edu_set)\n",
    "\tvalid_pos_words = set(valid_pos_words).intersection(pos_word_set)\n",
    "\t# print('pos_word_set', len(pos_word_set))\n",
    "\n",
    "\ttoken2vec = {}\n",
    "\tif sampling_size > 0:\n",
    "\t\trand_words = random.sample(valid_pos_words, sampling_size)\n",
    "\t\tfor word in rand_words:\n",
    "\t\t\ttoken2vec[word] = model.wv[word]\t\t\t\n",
    "\telse:\n",
    "\t\ttoken2vec = valid_pos_words\n",
    "\n",
    "\tidx = 0\n",
    "\tid2token = {}\n",
    "\ttoken2id = {}\n",
    "\n",
    "\tfor word in token2vec:\n",
    "\t    id2token[idx] = word\n",
    "\t    token2id[word] = idx\n",
    "\t    idx += 1\n",
    "\n",
    "\tvertices = [idx for idx in range(len(token2vec))]\n",
    "\tedges = [(i, j) for i in vertices for j in vertices if i < j]\n",
    "\tweights = [getDistance(id2token[i], id2token[j], model) for i, j in edges]\n",
    "\n",
    "\tg = filter_graph(weights, edges, vertices, id2token, max_dist)\n",
    "\n",
    "\treturn g, (edges, vertices), (id2token, token2id)\n",
    "\n",
    "def read_docs():\n",
    "\twith open(LEMMA_BOOK_FILE) as f:\n",
    "\t    documents = []\n",
    "\t    for line in f:\n",
    "\t        documents.append(line.split())\n",
    "\treturn documents\n",
    "\n",
    "def freq_counts(documents):\n",
    "    allwords = []\n",
    "    for doc in documents:\n",
    "        allwords += doc\n",
    "    \n",
    "    ct = Counter(allwords)\n",
    "    for i in range(5):\n",
    "        print('freq ', i+1, len([w for w in ct if ct[w] >= i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import *\n",
    "\n",
    "documents = read_docs()\n",
    "model = train_word2vec(documents, size=300, min_count=0, iters=500, window=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq  1 180076\n",
      "freq  2 60524\n",
      "freq  3 37660\n",
      "freq  4 28916\n",
      "freq  5 23983\n",
      "man woman 0.006240245420485735\n",
      "cat dog 0.06666990369558334\n",
      "coffee girl 0.20584052801132202\n"
     ]
    }
   ],
   "source": [
    "freq_counts(documents)\n",
    "print('man', 'woman', getDistance('man', 'woman', model))\n",
    "print('cat', 'dog', getDistance('cat', 'dog', model))\n",
    "print('coffee', 'girl', getDistance('coffee', 'girl', model))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlapping in Vocab\n",
    "    - therefore, we do not use google pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection b/w Google W2V, FB data, and PPVT is 165\n",
      "Intersection b/w FB data, and PPVT is 170\n"
     ]
    }
   ],
   "source": [
    "vocab_overlapping_googleW2V = len([w for w in ranked_words if w in set(model.wv.vocab)])\n",
    "print(\"Intersection b/w Google W2V, FB data, and PPVT is {}\".format(vocab_overlapping_googleW2V))\n",
    "\n",
    "book_set = set(book_words)\n",
    "vocab_overlapping_fb_data = len([w for w in ranked_words if w in book_set])\n",
    "print(\"Intersection b/w FB data, and PPVT is {}\".format(vocab_overlapping_fb_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "    - nouns\n",
    "    - verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_list(l, lower, upper):\n",
    "\tmax_min_range = max(l)-min(l)\n",
    "# \tprint('times', (upper-lower)/max_min_range)\n",
    "\treturn [lower + (upper - lower) / max_min_range * x for x in l]\n",
    "\n",
    "for i in range(0, 20, 1):\n",
    "\tmax_dist = 0.01*(i+1)\n",
    "\tg, (edges, vertices), (id2token, token2id) = build_pos_graph(model, pos_words['n'], sampling_size=10, max_dist=max_dist)\n",
    "\twordCounter = defaultdict(int)\n",
    "\twith open(LEMMA_BOOK_FILE) as f:\n",
    "\t    for line in f:\n",
    "\t        for w in line.split():\n",
    "\t            w = w.strip()\n",
    "\t            if w in token2id:\n",
    "\t                wordCounter[w] += 1\n",
    "\trank = [wordCounter[w] for w in token2id]\n",
    "\tvertexSize = normalize_list(rank, 10, 40)\n",
    "# \tvertexLabelSize = normalize_list(rank, 20, 40)\n",
    "\tvisual_style = {}\n",
    "\tvisual_style[\"vertex_color\"] = \"#94a1b6\"\n",
    "\tvisual_style[\"vertex_label_color\"] = \"#f20606\"\n",
    "\tvisual_style[\"vertex_label_size\"] = 15 # 15\n",
    "\tvisual_style[\"vertex_size\"] = vertexSize # 15\n",
    "\tvisual_style[\"vertex_label\"] = g.vs[\"label\"]\n",
    "\tlayout = g.layout(\"kk\")\n",
    "\tvisual_style[\"layout\"] = layout\n",
    "\tvisual_style[\"bbox\"] = (300, 300)\n",
    "\tvisual_style[\"margin\"] = 20\n",
    "\tfilename = \"./output/noun{}.pdf\".format(max_dist)\n",
    "\tplot(g, filename, **visual_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_list(l, lower, upper):\n",
    "\tmax_min_range = max(l)-min(l)\n",
    "# \tprint('times', (upper-lower)/max_min_range)\n",
    "\treturn [lower + (upper - lower) / max_min_range * x for x in l]\n",
    "\n",
    "for i in range(0, 20, 1):\n",
    "\tmax_dist = 0.01*(i+1)\n",
    "\tg, (edges, vertices), (id2token, token2id) = build_pos_graph(model, pos_words['v'], sampling_size=10, max_dist=max_dist)\n",
    "\twordCounter = defaultdict(int)\n",
    "\twith open(LEMMA_BOOK_FILE) as f:\n",
    "\t    for line in f:\n",
    "\t        for w in line.split():\n",
    "\t            w = w.strip()\n",
    "\t            if w in token2id:\n",
    "\t                wordCounter[w] += 1\n",
    "\trank = [wordCounter[w] for w in token2id]\n",
    "\tvertexSize = normalize_list(rank, 10, 40)\n",
    "# \tvertexLabelSize = normalize_list(rank, 20, 40)\n",
    "\tvisual_style = {}\n",
    "\tvisual_style[\"vertex_color\"] = \"#94a1b6\"\n",
    "\tvisual_style[\"vertex_label_color\"] = \"#f20606\"\n",
    "\tvisual_style[\"vertex_label_size\"] = 15 # 15\n",
    "\tvisual_style[\"vertex_size\"] = vertexSize # 15\n",
    "\tvisual_style[\"vertex_label\"] = g.vs[\"label\"]\n",
    "\tlayout = g.layout(\"kk\")\n",
    "\tvisual_style[\"layout\"] = layout\n",
    "\tvisual_style[\"bbox\"] = (300, 300)\n",
    "\tvisual_style[\"margin\"] = 20\n",
    "\tfilename = \"./output/noun{}.pdf\".format(max_dist)\n",
    "\tplot(g, filename, **visual_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rank words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec:\t 180076\n",
      "book_set:\t 43439\n",
      "edu_set:\t 228\n",
      "intersertion words 165\n",
      "      word  ppvt     strgth       close   betw     eigen  degree  freq\n",
      "108   ball     1   8.366802  381.662915  112.0  0.642104     141   391\n",
      "135    dog     2   7.112050  356.701514    5.0  0.503732     155  1327\n",
      "96   spoon     3   8.560623  293.250772   16.0  0.627558     145    87\n",
      "91    foot     4   8.871541  332.603761  226.0  0.671199     140  2389\n",
      "138   duck     5  10.060142  344.655486  424.0  0.731073     142   344\n",
      "rebecca vs. strgth -0.2786512069905362\n",
      "rebecca vs. close -0.045085512773500376\n",
      "rebecca vs. betw -0.01081914323232724\n",
      "rebecca vs. eigen -0.2821555588705194\n",
      "rebecca vs. degree 0.31483770531681465\n",
      "rebecca vs. freq -0.32370687637733453\n"
     ]
    }
   ],
   "source": [
    "# from model import *\n",
    "\n",
    "# documents = read_docs()\n",
    "\n",
    "# # no pretrained\n",
    "# model = train_word2vec(documents, 300, 5, 200)\n",
    "# # use pretrained model\n",
    "# # model = load_pretrained_word2vec(documents, 300, 5, 100, retrain=False)\n",
    "# # use pretrained + retrain\n",
    "# # model = load_pretrained_word2vec(documents, 300, 5, 30, retrain=True)\n",
    "\n",
    "# freq_counts(documents)\n",
    "# print('man', 'woman', getDistance('man', 'woman', model))\n",
    "# print('cat', 'dog', getDistance('cat', 'dog', model))\n",
    "# print('sun', 'kid', getDistance('sun', 'kid', model))        \n",
    "        \n",
    "# build graph     \n",
    "g, (edges, vertices), (id2token, token2id) = build_graph(model, sampling_size=10)\n",
    "\n",
    "# compute measures\n",
    "df = compute_measures_df(g, edges, vertices, id2token, token2id)\n",
    "\n",
    "# see correlations\n",
    "for col in df.columns[2:]:  \n",
    "    print('rebecca vs. {}'.format(col), df['ppvt'].corr(df[col]))\n",
    "\n",
    "# save to csv\n",
    "df.to_csv(\"./output/rankings_0603_170.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
