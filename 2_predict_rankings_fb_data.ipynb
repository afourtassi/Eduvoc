{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import * \n",
    "\n",
    "############## ONLY NEED TO CHANGE THE FOLLOWING TO USE preprocess module##########\n",
    "DIR = 'fb_data' \n",
    "BOOK_FILE = '{}/cbt.txt'.format(DIR)\n",
    "BOOK_LEMMA_FILE = '{}/cbt_lemma.txt'.format(DIR)\n",
    "PPVT_LEMMA_FILE = '{}/PPVT_lemma.csv'.format(DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n",
      "43439\n",
      "228\n"
     ]
    }
   ],
   "source": [
    "ranked_words= read_rebecca_lemma(PPVT_LEMMA_FILE)\n",
    "book_words, pos_words = lemmatize_book(BOOK_FILE, BOOK_LEMMA_FILE)\n",
    "generate_wordset_files(book_words, ranked_words, DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "    - Good to see increase in distance for these 3 pairs of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n",
    "from gensim.models import KeyedVectors\n",
    "from igraph import *\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\"\"\"\n",
    "task\n",
    "1. `model.py`: construct simple pipeline to build igraph, form df\n",
    "2. `preprocess.py`: build several graphs (V,N) in ipython book\n",
    "3. `visualize.py`: visualize network by sampling\n",
    "\"\"\"\n",
    "\n",
    "DIR = 'fb_data'\n",
    "# paths to the 3 generated files from preprocess.py\n",
    "BOOK_WORD_SET_FILE = '{}/book_words_set.txt'.format(DIR)\n",
    "REBECCA_WORD_FILE = '{}/rebecca_words.txt'.format(DIR)\n",
    "LEMMA_BOOK_FILE = \"{}/cbt_lemma.txt\".format(DIR)\n",
    "\n",
    "def read_file_to_list(filename):\n",
    "\twords = []\n",
    "\twith open(filename, 'r') as f:\n",
    "\t    for word in f:\n",
    "\t        words.append(word.strip())\n",
    "\treturn words\n",
    "\n",
    "def getSimilarity(w1, w2, model):\n",
    "    return abs(1-spatial.distance.cosine(model.wv[w1], model.wv[w2]))\n",
    "\n",
    "def load_pretrained_word2vec(documents, size, min_count, iters, retrain=False):\n",
    "\tGOOGLE_W2V_FILE = './model/GoogleNews-vectors-negative300.bin'\n",
    "\tmodel = KeyedVectors.load_word2vec_format(GOOGLE_W2V_FILE, binary=True)  \n",
    "\tif not retrain:\n",
    "\t\treturn model\n",
    "\tmodel_2 = gensim.models.Word2Vec(\n",
    "\t            documents,\n",
    "\t            sg=0,\n",
    "\t            size=size,\n",
    "\t            window=20,\n",
    "\t            min_count=min_count,\n",
    "\t            iter=iters,\n",
    "\t            workers=4)\n",
    "\ttotal_examples = model_2.corpus_count\n",
    "\tmodel_2.build_vocab([list(model.vocab.keys())], update=True)\n",
    "\tmodel_2.intersect_word2vec_format(GOOGLE_W2V_FILE, binary=True, lockf=1.0)\n",
    "\tmodel_2.train(documents, total_examples=total_examples, epochs=model_2.epochs)\n",
    "\treturn model_2\n",
    "\n",
    "def train_word2vec(documents, size=300, min_count=5, iters=100, window=20):\n",
    "\tmodel = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        sg=0,\n",
    "        size=size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        iter=iters,\n",
    "        workers=4)\n",
    "\tmodel.train(documents, total_examples=len(documents), epochs=model.epochs)\n",
    "\treturn model\n",
    "\n",
    "def build_graph(model, sampling_size=0):\n",
    "\n",
    "\tbook_set = read_file_to_list(BOOK_WORD_SET_FILE)\n",
    "\tedu_set = read_file_to_list(REBECCA_WORD_FILE)\n",
    "\n",
    "\tprint(\"word2vec:\\t\", len(model.wv.vocab))\n",
    "\tprint(\"book_set:\\t\", len(book_set))\n",
    "\tprint(\"edu_set:\\t\", len(edu_set))\n",
    "# \tsurrounding_words = set(model.wv.vocab.keys()).intersection(book_set)\n",
    "# \tsurrounding_words = set(surrounding_words).difference(edu_set)\n",
    "# \tprint('surrounding words', len(surrounding_words))\n",
    "\tintersection_words = set(model.wv.vocab.keys()).intersection(edu_set)\n",
    "\tprint('intersertion words', len(intersection_words))\n",
    "\n",
    "\tif sampling_size:\n",
    "\t\ttoken2vec = {}\n",
    "\t\trand_words = random.sample(intersection_words, sampling_size)\n",
    "\t\tfor word in rand_words:\n",
    "\t\t\ttoken2vec[word] = model.wv[word]\n",
    "\t\tfor word in intersection_words:\n",
    "\t\t\ttoken2vec[word] = model.wv[word]\n",
    "\telse:\n",
    "\t\ttoken2vec = model.wv.vocab # bad\n",
    "\n",
    "\tidx = 0\n",
    "\tid2token = {}\n",
    "\ttoken2id = {}\n",
    "\n",
    "\tfor word in token2vec:\n",
    "\t    id2token[idx] = word\n",
    "\t    token2id[word] = idx\n",
    "\t    idx += 1\n",
    "\n",
    "\tvertices = [idx for idx in range(len(token2vec))]\n",
    "\tedges = [(i, j) for i in vertices for j in vertices if i < j]\n",
    "\tg = Graph(vertex_attrs={\"label\":vertices}, edges=edges, directed=False)\n",
    "\tg.es[\"sim\"] = [getSimilarity(id2token[i], id2token[j], model) for i,j in edges]\n",
    "\tg.es[\"dist\"] = np.array(1-np.array(g.es['sim'])).tolist()\n",
    "\t# test validity\n",
    "\t# assert(getSimilarity('man', 'woman', model) == g[token2id['man'], token2id['woman']] )\n",
    "\t# assert(getSimilarity('cat', 'dog', model) == g[token2id['cat'], token2id['dog']] )\n",
    "\n",
    "\treturn g, (edges, vertices), (id2token, token2id)\n",
    "\n",
    "def filter_graph(weights, edges, vertices, id2token,  threshold=1.0):\n",
    "\n",
    "\tnew_edges, new_weights = [], [] # weights: similarity\n",
    "\tnew_distances = []\n",
    "\tfor edge, w in zip(edges, weights):\n",
    "\t    if w >= threshold:\n",
    "\t        new_edges.append(edge)\n",
    "\t        new_weights.append(w)\n",
    "\t        new_distances.append(1-w)\n",
    "\tg0 = Graph(vertex_attrs={\"label\":vertices}, edges=new_edges, directed=False)\n",
    "\n",
    "\tg0.es[\"sim\"] = new_weights\n",
    "\tg0.es[\"dist\"] = new_distances\n",
    "\tg0.vs[\"label\"] = [id2token[idx] for idx in vertices]\n",
    "\n",
    "\treturn g0\n",
    "\n",
    "def compute_measures_df(g, edges, vertices, id2token, token2id):\n",
    "\t\"\"\"\n",
    "\tcompute centrality measures to output df\n",
    "\t\"\"\"\n",
    "\n",
    "\t# g -> strength, closeness (continuous)\n",
    "\tstrengthRank = g.strength(None,  weights=g.es['sim'])\n",
    "\tclosenessRank = g.closeness(None, 'all', weights=g.es['dist'], normalized=True)\n",
    "\n",
    "\t# g1 -> betweenness, eigen_centrality\n",
    "\tg1 = filter_graph(g.es[\"sim\"], edges, vertices, id2token, threshold=0.5)\n",
    "    \n",
    "\teigen_centralityRank = g.eigenvector_centrality(directed=False, weights=g.es['sim'])\n",
    "\n",
    "\tbetweennessRank = g.betweenness(vertices=None, \n",
    "                                     directed=False, \n",
    "                                     weights=g.es['dist'])\n",
    "\n",
    "\t# g2 -> degree\n",
    "\tg2 = filter_graph(g.es[\"sim\"], edges, vertices, id2token, threshold=0.1)\n",
    "\n",
    "\tdegreeRank = g2.degree(None, mode='all')\n",
    "\tprint(degreeRank)\n",
    "\n",
    "\t# frequency\n",
    "\twordCounter = defaultdict(int)\n",
    "\twith open(LEMMA_BOOK_FILE) as f:\n",
    "\t    for line in f:\n",
    "\t        for w in line.split():\n",
    "\t            w = w.strip()\n",
    "\t            if w in token2id:\n",
    "\t                wordCounter[w] += 1\n",
    "\tfreqRank = [wordCounter[w] for w in token2id]\n",
    "\n",
    "\t# rebecca\n",
    "\twith open(REBECCA_WORD_FILE, 'r') as f:\n",
    "\t    edu_list = []\n",
    "\t    for word in f:\n",
    "\t        edu_list.append(word.strip())\n",
    "\t        \n",
    "\trebeccaRank = {w:i+1 for i, w in enumerate(edu_list)}\n",
    "\tfinal_words_set = set(token2id.keys()).intersection(edu_list)\n",
    "\n",
    "\tprint(len(strengthRank))\n",
    "\tprint(len(betweennessRank))\n",
    "\tprint(len(closenessRank))\n",
    "\tprint(len(eigen_centralityRank))\n",
    "\tprint(len(degreeRank))\n",
    "\n",
    "\t# create df\n",
    "\tdata = []\n",
    "\twords_inorder = [id2token[idx] for idx in range(len(token2id))]\n",
    "\tfor i, word in enumerate(words_inorder):\n",
    "\t    if word in final_words_set:\n",
    "\t        data.append([word,\n",
    "\t                    rebeccaRank[word],\n",
    "\t                    strengthRank[i], \n",
    "\t                    closenessRank[i], \n",
    "\t                    betweennessRank[i], \n",
    "\t                    eigen_centralityRank[i],\n",
    "\t                    degreeRank[i],\n",
    "\t                    freqRank[i]])\n",
    "\n",
    "\n",
    "\tdf = pd.DataFrame(data, columns=['word', 'ppvt', 'strgth', \n",
    "                                     'close', 'betw', 'eigen', 'degree', 'freq'])\n",
    "\tdf = df.sort_values(by=['ppvt'])\n",
    "\tprint(df.head())\n",
    "\treturn df\n",
    "\n",
    "def build_pos_graph(model, pos_word_set, sampling_size=0, max_dist=1):\n",
    "\t\"\"\"\n",
    "\tbuild semantic graph without referring Rebecca's word list;\n",
    "\tonly to explore noun, verb's centrality in vocab\n",
    "\t\"\"\"\n",
    "\n",
    "\tbook_set = read_file_to_list(BOOK_WORD_SET_FILE)\n",
    "\tedu_set = read_file_to_list(REBECCA_WORD_FILE)\n",
    "\tvalid_pos_words = set(model.wv.vocab.keys()).intersection(edu_set)\n",
    "\tvalid_pos_words = set(valid_pos_words).intersection(pos_word_set)\n",
    "\t# print('pos_word_set', len(pos_word_set))\n",
    "\n",
    "\ttoken2vec = {}\n",
    "\tif sampling_size > 0:\n",
    "\t\trand_words = random.sample(valid_pos_words, sampling_size)\n",
    "\t\tfor word in rand_words:\n",
    "\t\t\ttoken2vec[word] = model.wv[word]\t\t\t\n",
    "\telse:\n",
    "\t\ttoken2vec = valid_pos_words\n",
    "\n",
    "\tidx = 0\n",
    "\tid2token = {}\n",
    "\ttoken2id = {}\n",
    "\n",
    "\tfor word in token2vec:\n",
    "\t    id2token[idx] = word\n",
    "\t    token2id[word] = idx\n",
    "\t    idx += 1\n",
    "\n",
    "\tvertices = [idx for idx in range(len(token2vec))]\n",
    "\tedges = [(i, j) for i in vertices for j in vertices if i < j]\n",
    "\tweights = [getSimilarity(id2token[i], id2token[j], model) for i, j in edges]\n",
    "\n",
    "\tg = filter_graph(weights, edges, vertices, id2token, max_dist)\n",
    "\n",
    "\treturn g, (edges, vertices), (id2token, token2id)\n",
    "\n",
    "def read_docs():\n",
    "\twith open(LEMMA_BOOK_FILE) as f:\n",
    "\t    documents = []\n",
    "\t    for line in f:\n",
    "\t        documents.append(line.split())\n",
    "\treturn documents\n",
    "\n",
    "def freq_counts(documents):\n",
    "    allwords = []\n",
    "    for doc in documents:\n",
    "        allwords += doc\n",
    "    \n",
    "    ct = Counter(allwords)\n",
    "    for i in range(5):\n",
    "        print('freq ', i+1, len([w for w in ct if ct[w] >= i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import *\n",
    "\n",
    "documents = read_docs()\n",
    "model = train_word2vec(documents, size=100, min_count=0, iters=50, window=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq  1 180076\n",
      "freq  2 60524\n",
      "freq  3 37660\n",
      "freq  4 28916\n",
      "freq  5 23983\n",
      "table girl 0.27819833159446716\n",
      "man woman 0.7985084056854248\n",
      "girl boy 0.4241322875022888\n",
      "sun moon 0.7583851218223572\n",
      "cat dog 0.022278811782598495\n"
     ]
    }
   ],
   "source": [
    "freq_counts(documents)\n",
    "print('table', 'girl', getSimilarity('table', 'girl', model))\n",
    "print('man', 'woman', getSimilarity('man', 'woman', model))\n",
    "print('girl', 'boy', getSimilarity('girl', 'boy', model))\n",
    "print('sun', 'moon', getSimilarity('sun', 'moon', model))\n",
    "print('cat', 'dog', getSimilarity('cat', 'dog', model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlapping in Vocab\n",
    "    - therefore, we do not use google pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection b/w Google W2V, FB data, and PPVT is 165\n",
      "Intersection b/w FB data, and PPVT is 170\n"
     ]
    }
   ],
   "source": [
    "vocab_overlapping_googleW2V = len([w for w in ranked_words if w in set(model.wv.vocab)])\n",
    "print(\"Intersection b/w Google W2V, FB data, and PPVT is {}\".format(vocab_overlapping_googleW2V))\n",
    "\n",
    "book_set = set(book_words)\n",
    "vocab_overlapping_fb_data = len([w for w in ranked_words if w in book_set])\n",
    "print(\"Intersection b/w FB data, and PPVT is {}\".format(vocab_overlapping_fb_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "    - nouns\n",
    "    - verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_list(l, lower, upper):\n",
    "\tmax_min_range = max(l)-min(l)\n",
    "# \tprint('times', (upper-lower)/max_min_range)\n",
    "\treturn [lower + (upper - lower) / max_min_range * x for x in l]\n",
    "\n",
    "for i in range(0, 20, 1):\n",
    "\tmax_dist = 0.01*(i+1)\n",
    "\tg, (edges, vertices), (id2token, token2id) = build_pos_graph(model, pos_words['n'], sampling_size=10, max_dist=max_dist)\n",
    "\twordCounter = defaultdict(int)\n",
    "\twith open(LEMMA_BOOK_FILE) as f:\n",
    "\t    for line in f:\n",
    "\t        for w in line.split():\n",
    "\t            w = w.strip()\n",
    "\t            if w in token2id:\n",
    "\t                wordCounter[w] += 1\n",
    "\trank = [wordCounter[w] for w in token2id]\n",
    "\tvertexSize = normalize_list(rank, 10, 40)\n",
    "# \tvertexLabelSize = normalize_list(rank, 20, 40)\n",
    "\tvisual_style = {}\n",
    "\tvisual_style[\"vertex_color\"] = \"#94a1b6\"\n",
    "\tvisual_style[\"vertex_label_color\"] = \"#f20606\"\n",
    "\tvisual_style[\"vertex_label_size\"] = 15 # 15\n",
    "\tvisual_style[\"vertex_size\"] = vertexSize # 15\n",
    "\tvisual_style[\"vertex_label\"] = g.vs[\"label\"]\n",
    "\tlayout = g.layout(\"kk\")\n",
    "\tvisual_style[\"layout\"] = layout\n",
    "\tvisual_style[\"bbox\"] = (300, 300)\n",
    "\tvisual_style[\"margin\"] = 20\n",
    "\tfilename = \"./output/noun{}.pdf\".format(max_dist)\n",
    "\tplot(g, filename, **visual_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_list(l, lower, upper):\n",
    "\tmax_min_range = max(l)-min(l)\n",
    "# \tprint('times', (upper-lower)/max_min_range)\n",
    "\treturn [lower + (upper - lower) / max_min_range * x for x in l]\n",
    "\n",
    "for i in range(0, 20, 1):\n",
    "\tmax_dist = 0.01*(i+1)\n",
    "\tg, (edges, vertices), (id2token, token2id) = build_pos_graph(model, pos_words['v'], sampling_size=10, max_dist=max_dist)\n",
    "\twordCounter = defaultdict(int)\n",
    "\twith open(LEMMA_BOOK_FILE) as f:\n",
    "\t    for line in f:\n",
    "\t        for w in line.split():\n",
    "\t            w = w.strip()\n",
    "\t            if w in token2id:\n",
    "\t                wordCounter[w] += 1\n",
    "\trank = [wordCounter[w] for w in token2id]\n",
    "\tvertexSize = normalize_list(rank, 10, 40)\n",
    "# \tvertexLabelSize = normalize_list(rank, 20, 40)\n",
    "\tvisual_style = {}\n",
    "\tvisual_style[\"vertex_color\"] = \"#94a1b6\"\n",
    "\tvisual_style[\"vertex_label_color\"] = \"#f20606\"\n",
    "\tvisual_style[\"vertex_label_size\"] = 15 # 15\n",
    "\tvisual_style[\"vertex_size\"] = vertexSize # 15\n",
    "\tvisual_style[\"vertex_label\"] = g.vs[\"label\"]\n",
    "\tlayout = g.layout(\"kk\")\n",
    "\tvisual_style[\"layout\"] = layout\n",
    "\tvisual_style[\"bbox\"] = (300, 300)\n",
    "\tvisual_style[\"margin\"] = 20\n",
    "\tfilename = \"./output/noun{}.pdf\".format(max_dist)\n",
    "\tplot(g, filename, **visual_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rank words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec:\t 180076\n",
      "book_set:\t 43439\n",
      "edu_set:\t 228\n",
      "intersertion words 165\n",
      "[59, 55, 65, 67, 48, 41, 54, 75, 58, 49, 43, 48, 41, 71, 80, 43, 43, 51, 48, 75, 73, 46, 65, 48, 73, 55, 54, 53, 42, 81, 66, 34, 58, 67, 54, 44, 50, 62, 57, 68, 46, 59, 49, 71, 62, 73, 36, 70, 59, 55, 75, 42, 62, 49, 70, 53, 42, 48, 56, 41, 61, 59, 42, 44, 52, 44, 73, 54, 76, 49, 72, 40, 61, 48, 37, 49, 42, 81, 48, 42, 49, 73, 49, 64, 76, 50, 59, 49, 38, 46, 52, 44, 75, 80, 51, 43, 55, 70, 68, 47, 76, 46, 73, 49, 43, 42, 61, 33, 57, 52, 54, 63, 51, 54, 45, 48, 44, 59, 82, 63, 50, 61, 44, 49, 78, 66, 61, 66, 44, 46, 69, 51, 59, 57, 55, 42, 45, 47, 70, 74, 58, 54, 76, 47, 77, 70, 60, 38, 51, 47, 73, 41, 58, 56, 53, 51, 43, 54, 65, 59, 58, 66, 55, 71, 64]\n",
      "165\n",
      "165\n",
      "165\n",
      "165\n",
      "165\n",
      "      word  ppvt     strgth     close  betw     eigen  degree  freq\n",
      "47    ball     1  27.373324  1.230778   6.0  0.786280      70   391\n",
      "105    dog     2  12.242857  1.080674   0.0  0.166297      42  1327\n",
      "75   spoon     3  13.525252  1.089972   0.0  0.235678      49    87\n",
      "139   foot     4  31.645910  1.252969  17.0  0.941987      74  2389\n",
      "93    duck     5  31.847777  1.261390  27.0  0.939853      80   344\n",
      "rebecca vs. strgth 0.1684781317626651\n",
      "rebecca vs. close 0.1691387493605582\n",
      "rebecca vs. betw 0.2304797306463518\n",
      "rebecca vs. eigen 0.19690326834200744\n",
      "rebecca vs. degree 0.15837348287569783\n",
      "rebecca vs. freq 0.5445484208190793\n"
     ]
    }
   ],
   "source": [
    "# from model import *\n",
    "\n",
    "# documents = read_docs()\n",
    "\n",
    "# # no pretrained\n",
    "# model = train_word2vec(documents, 300, 5, 200)\n",
    "# # use pretrained model\n",
    "# # model = load_pretrained_word2vec(documents, 300, 5, 100, retrain=False)\n",
    "# # use pretrained + retrain\n",
    "# # model = load_pretrained_word2vec(documents, 300, 5, 30, retrain=True)\n",
    "\n",
    "# freq_counts(documents)\n",
    "# print('man', 'woman', getDistance('man', 'woman', model))\n",
    "# print('cat', 'dog', getDistance('cat', 'dog', model))\n",
    "# print('sun', 'kid', getDistance('sun', 'kid', model))        \n",
    "        \n",
    "# build graph     \n",
    "g, (edges, vertices), (id2token, token2id) = build_graph(model, sampling_size=10)\n",
    "\n",
    "# compute measures\n",
    "df = compute_measures_df(g, edges, vertices, id2token, token2id)\n",
    "\n",
    "# see correlations\n",
    "for col in df.columns[2:]:  \n",
    "    print('rebecca vs. {}'.format(col), df['ppvt'].corr(df[col].rank(ascending=False)))    \n",
    "    \n",
    "for mname in ['strgth', 'close', 'betw', 'eigen', 'degree', 'freq']:\n",
    "    cname = mname + '-rank'\n",
    "    df[cname] = df[mname].rank(ascending=False)    \n",
    "\n",
    "# save to csv\n",
    "df.to_csv(\"./output/rankings_0625_170.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ppvt</th>\n",
       "      <th>strgth</th>\n",
       "      <th>close</th>\n",
       "      <th>betw</th>\n",
       "      <th>eigen</th>\n",
       "      <th>degree</th>\n",
       "      <th>freq</th>\n",
       "      <th>strgth-rank</th>\n",
       "      <th>close-rank</th>\n",
       "      <th>betw-rank</th>\n",
       "      <th>eigen-rank</th>\n",
       "      <th>degree-rank</th>\n",
       "      <th>freq-rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ball</td>\n",
       "      <td>1</td>\n",
       "      <td>27.373324</td>\n",
       "      <td>1.230778</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.786280</td>\n",
       "      <td>70</td>\n",
       "      <td>391</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>dog</td>\n",
       "      <td>2</td>\n",
       "      <td>12.242857</td>\n",
       "      <td>1.080674</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166297</td>\n",
       "      <td>42</td>\n",
       "      <td>1327</td>\n",
       "      <td>134.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>150.5</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>spoon</td>\n",
       "      <td>3</td>\n",
       "      <td>13.525252</td>\n",
       "      <td>1.089972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235678</td>\n",
       "      <td>49</td>\n",
       "      <td>87</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>109.5</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>foot</td>\n",
       "      <td>4</td>\n",
       "      <td>31.645910</td>\n",
       "      <td>1.252969</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.941987</td>\n",
       "      <td>74</td>\n",
       "      <td>2389</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>duck</td>\n",
       "      <td>5</td>\n",
       "      <td>31.847777</td>\n",
       "      <td>1.261390</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.939853</td>\n",
       "      <td>80</td>\n",
       "      <td>344</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  ppvt     strgth     close  betw     eigen  degree  freq  \\\n",
       "47    ball     1  27.373324  1.230778   6.0  0.786280      70   391   \n",
       "105    dog     2  12.242857  1.080674   0.0  0.166297      42  1327   \n",
       "75   spoon     3  13.525252  1.089972   0.0  0.235678      49    87   \n",
       "139   foot     4  31.645910  1.252969  17.0  0.941987      74  2389   \n",
       "93    duck     5  31.847777  1.261390  27.0  0.939853      80   344   \n",
       "\n",
       "     strgth-rank  close-rank  betw-rank  eigen-rank  degree-rank  freq-rank  \n",
       "47          23.0        19.0       19.5        21.0         30.0       21.0  \n",
       "105        134.0       135.0       94.0       131.0        150.5        7.0  \n",
       "75          72.0        72.0       94.0        58.0        109.5       53.0  \n",
       "139          7.0         9.0       12.5         6.0         16.0        2.0  \n",
       "93           4.0         3.0       10.0         7.0          4.5       26.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
