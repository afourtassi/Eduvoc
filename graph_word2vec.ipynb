{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a few things to do\n",
    "- process the word list\n",
    "- create the igraph\n",
    "- generate rank dataframe for Abdell to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construct word2vec from book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "with open('./data/lemma_book.txt') as f:\n",
    "    documents = []\n",
    "    for line in f:\n",
    "        documents.append(line.split())\n",
    "# print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize and validate the word embeddings\n",
    "    - tweaking iter, epoch improves quality of word embeddings\n",
    "    - use iter=500, epoch=20, for now\n",
    "    - trained only on book data (no external/pre-trained knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=2828, size=300, alpha=0.025)\n",
      "------ 20 iters ----------\n",
      "0.6432896256446838\n",
      "0.9624505639076233\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "model = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        sg=0,\n",
    "        size=300,\n",
    "        window=20,\n",
    "        min_count=2,\n",
    "        iter=20, # this is important, b/c we want overfitting\n",
    "        workers=4)\n",
    "model.train(documents, total_examples=len(documents), epochs=20)\n",
    "print(model)\n",
    "\n",
    "def getDistance(w1, w2):\n",
    "    return abs(1-spatial.distance.cosine(model.wv[w1], model.wv[w2]))\n",
    "\n",
    "print('------ 20 iters ----------')\n",
    "print(getDistance('man', 'woman'))\n",
    "print(getDistance('cat', 'dog'))\n",
    "# print(getDistance('king', 'queen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=2828, size=300, alpha=0.025)\n",
      "------ 100 iters ----------\n",
      "0.10735931247472763\n",
      "0.6275115013122559\n",
      "0.10306362807750702\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "model = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        sg=0,\n",
    "        size=300,\n",
    "        window=20,\n",
    "        min_count=2,\n",
    "        iter=100, # this is important, b/c we want overfitting\n",
    "        workers=4)\n",
    "model.train(documents, total_examples=len(documents), epochs=20)\n",
    "print(model)\n",
    "\n",
    "def getDistance(w1, w2):\n",
    "    return abs(1-spatial.distance.cosine(model.wv[w1], model.wv[w2]))\n",
    "\n",
    "print('------ 100 iters ----------')\n",
    "print(getDistance('man', 'woman'))\n",
    "print(getDistance('cat', 'dog'))\n",
    "print(getDistance('king', 'queen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=2828, size=300, alpha=0.025)\n",
      "------ 300 iters ----------\n",
      "0.1841917634010315\n",
      "0.44950127601623535\n",
      "0.10599736869335175\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "model = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        sg=0,\n",
    "        size=300,\n",
    "        window=20,\n",
    "        min_count=2,\n",
    "        iter=300, # this is important, b/c we want overfitting\n",
    "        workers=4)\n",
    "model.train(documents, total_examples=len(documents), epochs=20)\n",
    "print(model)\n",
    "\n",
    "def getDistance(w1, w2):\n",
    "    return abs(1-spatial.distance.cosine(model.wv[w1], model.wv[w2]))\n",
    "\n",
    "print('------ 300 iters ----------')\n",
    "print(getDistance('man', 'woman'))\n",
    "print(getDistance('cat', 'dog'))\n",
    "print(getDistance('king', 'queen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=1224, size=300, alpha=0.025)\n",
      "------ 500 iters ----------\n",
      "0.06545022875070572\n",
      "0.3772120177745819\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "model = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        sg=0,\n",
    "        size=300,\n",
    "        window=20,\n",
    "        min_count=5,\n",
    "        iter=500, # this is important, b/c we want overfitting\n",
    "        workers=4)\n",
    "model.train(documents, total_examples=len(documents), epochs=20)\n",
    "print(model)\n",
    "\n",
    "def getDistance(w1, w2):\n",
    "    return abs(1-spatial.distance.cosine(model.wv[w1], model.wv[w2]))\n",
    "\n",
    "print('------ 500 iters ----------')\n",
    "print(getDistance('man', 'woman'))\n",
    "print(getDistance('cat', 'dog'))\n",
    "# print(getDistance('king', 'queen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save & load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "path = get_tmpfile(\"word2vec.model\")\n",
    "model.save(\"word2vec.model\")\n",
    "model = gensim.models.Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build igraph using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec:\t 1224\n",
      "book_set:\t 4751\n",
      "edu_set:\t 228\n"
     ]
    }
   ],
   "source": [
    "# TODO: examine the word2vec effectiveness using visualizations\n",
    "\n",
    "# with open('data/intersection.txt', 'r') as f:\n",
    "#     inter_set = []\n",
    "#     for word in f:\n",
    "#         inter_set.append(word)\n",
    "\n",
    "with open('data/book_words_set.txt', 'r') as f:\n",
    "    book_set = []\n",
    "    for word in f:\n",
    "        book_set.append(word)\n",
    "\n",
    "with open('data/rebecca_words.txt', 'r') as f:\n",
    "    edu_set = []\n",
    "    for word in f:\n",
    "        edu_set.append(word)\n",
    "        \n",
    "print(\"word2vec:\\t\", len(model.wv.vocab))\n",
    "# print(\"inter_set:\\t\", len(inter_set))\n",
    "print(\"book_set:\\t\", len(book_set))\n",
    "print(\"edu_set:\\t\", len(edu_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct igraph based on word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from igraph import *\n",
    "# token2vec\n",
    "token2vec = model.wv.vocab\n",
    "# create id2token\n",
    "idx = 0\n",
    "id2token = {}\n",
    "token2id = {}\n",
    "words = token2id\n",
    "\n",
    "for word in token2vec:\n",
    "    id2token[idx] = word\n",
    "    token2id[word] = idx\n",
    "    idx += 1\n",
    "\n",
    "vertices = [idx for idx in range(len(token2vec))]\n",
    "edges = [(i, j) for i in vertices for j in vertices if i < j]\n",
    "g = Graph(vertex_attrs={\"label\":vertices}, edges=edges, directed=False)\n",
    "    \n",
    "g.vs[\"word\"] = token2id.keys()\n",
    "weights = [getDistance(id2token[i], id2token[j]) for i, j in edges]\n",
    "g.es[\"weight\"] = weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the validity of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(getDistance('man', 'woman') == g[token2id['man'], token2id['woman']] )\n",
    "assert(getDistance('cat', 'dog') == g[token2id['cat'], token2id['dog']] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality Measures of graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[38.75003078268492,\n",
       " 42.48841784886463,\n",
       " 38.917647697952816,\n",
       " 42.114721707424906,\n",
       " 35.994142794017534,\n",
       " 43.51131318032276,\n",
       " 42.20643922920863,\n",
       " 40.37464757099042,\n",
       " 44.98500001744469,\n",
       " 37.89474887335746]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strengthRank = g2.strength(None,  weights=g2.es['weight'])\n",
    "strengthRank[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1808.958927083432,\n",
       " 1749.6477087074572,\n",
       " 1915.9969874838953,\n",
       " 1621.9277408499327,\n",
       " 1773.914299832439,\n",
       " 1484.6054957951096,\n",
       " 1341.230963977371,\n",
       " 1902.5358331617228,\n",
       " 1674.4189310987413,\n",
       " 1420.9826123979083]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closenessRank = g2.closeness(None, 'all', weights=g2.es['weight'], normalized=True)\n",
    "closenessRank[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### betweenness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove edges whose dsitance >= 0.5\n",
    "max_dist=0.5\n",
    "\n",
    "new_edges, new_weights = [], []\n",
    "for edge, dist in zip(edges, weights):\n",
    "    if dist < max_dist:\n",
    "        new_edges.append(edge)\n",
    "        new_weights.append(dist)\n",
    "g1 = Graph(vertex_attrs={\"label\":vertices}, edges=new_edges, directed=False)\n",
    "g1.vs[\"word\"] = words.keys()\n",
    "g1.es[\"weight\"] = new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1500.0, 2343.0, 11210.0, 1579.0, 6948.0, 580.0, 429.0, 15632.0, 6533.0, 160.0]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betweennessRank = g2.betweenness(directed=False, weights=g2.es['weight'])\n",
    "betweennessRank[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eigen_centrality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8308252745300805,\n",
       " 0.9072724801467226,\n",
       " 0.8328147082121062,\n",
       " 0.8985276867293528,\n",
       " 0.7696499887426691,\n",
       " 0.9284151293181844,\n",
       " 0.9009343249756069,\n",
       " 0.8602218363028828,\n",
       " 0.956036647686703,\n",
       " 0.8131211224894905]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigen_centralityRank = g2.eigenvector_centrality(directed=False, weights=g2.es['weight'])\n",
    "eigen_centralityRank[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove edges\n",
    "max_dist=0.1\n",
    "\n",
    "new_edges, new_weights = [], []\n",
    "for edge, dist in zip(edges, weights):\n",
    "    if dist < max_dist:\n",
    "        new_edges.append(edge)\n",
    "        new_weights.append(dist)\n",
    "g2 = Graph(vertex_attrs={\"label\":vertices}, edges=new_edges, directed=False)\n",
    "\n",
    "\n",
    "g2.vs[\"word\"] = words.keys()\n",
    "g2.es[\"weight\"] = new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[878, 957, 874, 1023, 848, 1020, 965, 1058, 1098, 869]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degreeRank = g2.degree(mode='all')\n",
    "degreeRank[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 147, 9, 419, 67, 686, 109, 1466, 3269, 26]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem with frequency (only counts words in the graph)\n",
    "from collections import defaultdict\n",
    "wordCounter = defaultdict(int)\n",
    "with open(\"data/lemma_book.txt\") as f:\n",
    "    for line in f:\n",
    "        for w in line.split():\n",
    "            w = w.strip()\n",
    "            if w in token2id:\n",
    "                wordCounter[w] += 1\n",
    "freqRank = [wordCounter[w] for w in token2id]\n",
    "freqRank[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate dataframe for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of final words:\t 39\n",
      "{'ball': 1, 'dog': 2, 'spoon': 3, 'foot': 4, 'duck': 5, 'banana': 6, 'shoe': 7, 'cup': 8, 'eat': 9, 'bus': 10, 'flower': 11, 'mouth': 12, 'pencil': 13, 'cookie': 14, 'drum': 15, 'turtle': 16, 'red': 17, 'jump': 18, 'carrot': 19, 'read': 20, 'toe': 21, 'belt': 22, 'fly': 23, 'paint': 24, 'dance': 25, 'whistle': 26, 'kick': 27, 'lamp': 28, 'square': 29, 'fence': 30, 'empty': 31, 'happy': 32, 'fire': 33, 'castle': 34, 'squirrel': 35, 'throw': 36, 'farm': 37, 'penguin': 38, 'gift': 39, 'feather': 40, 'cobweb': 41, 'elbow': 42, 'juggle': 43, 'fountain': 44, 'net': 45, 'shoulder': 46, 'dress': 47, 'roof': 48, 'peek': 49, 'ruler': 50, 'tunnel': 51, 'branch': 52, 'envelope': 53, 'diamond': 54, 'calender': 55, 'buckle': 56, 'sawing': 57, 'panda': 58, 'vest': 59, 'arrow': 60, 'pick': 61, 'target': 62, 'drip': 63, 'knight': 64, 'deliver': 65, 'cactus': 66, 'dentist': 67, 'float': 68, 'claw': 69, 'uniform': 70, 'gigantic': 71, 'furry': 72, 'violin': 73, 'group': 74, 'globe': 75, 'vehicle': 76, 'chef': 77, 'squash': 78, 'ax': 79, 'flamingo': 80, 'chimney': 81, 'sort': 82, 'waist': 83, 'vegetable': 84, 'hyena': 85, 'plumber': 86, 'river': 87, 'timer': 88, 'catch': 89, 'trunk': 90, 'vase': 91, 'harp': 92, 'bloom': 93, 'horrified': 94, 'swamp': 95, 'heart': 96, 'pigeon': 97, 'ankle': 98, 'flaming': 99, 'wrench': 100, 'aquarium': 101, 'refuel': 102, 'safe': 103, 'boulder': 104, 'reptile': 105, 'canoe': 106, 'athlete': 107, 'towing': 108, 'luggage': 109, 'direct': 110, 'vine': 111, 'digital': 112, 'dissect': 113, 'predatory': 114, 'hydrant': 115, 'surprised': 116, 'palm': 117, 'clarinet': 118, 'valley': 119, 'kiwi': 120, 'interview': 121, 'pastry': 122, 'assist': 123, 'fragile': 124, 'solo': 125, 'snarl': 126, 'puzzled': 127, 'beverage': 128, 'inflated': 129, 'tusk': 130, 'trumpet': 131, 'rodent': 132, 'inhale': 133, 'links': 134, 'pollute': 135, 'archaeologist': 136, 'coast': 137, 'inject': 138, 'fern': 139, 'mammal': 140, 'demolish': 141, 'isolation': 142, 'clamp': 143, 'dilapidated': 144, 'pedestrian': 145, 'interior': 146, 'garment': 147, 'depart': 148, 'feline': 149, 'hedge': 150, 'citrus': 151, 'florist': 152, 'hover': 153, 'aquatic': 154, 'reprimand': 155, 'carpenter': 156, 'primate': 157, 'glider': 158, 'weary': 159, 'hatchet': 160, 'transparent': 161, 'sedan': 162, 'constrained': 163, 'valve': 164, 'parallelogram': 165, 'pillar': 166, 'consume': 167, 'currency': 168, 'hazardous': 169, 'pentagon': 170, 'appliance': 171, 'poultry': 172, 'cornea': 173, 'peninsula': 174, 'porcelain': 175, 'detonation': 176, 'cerebral': 177, 'perpendicular': 178, 'submerge': 179, 'syringe': 180, 'lever': 181, 'apparel': 182, 'talon': 183, 'cultivate': 184, 'wedge': 185, 'ascend': 186, 'depleted': 187, 'sternum': 188, 'maritime': 189, 'incarcerate': 190, 'dejected': 191, 'quintet': 192, 'incandescent': 193, 'confide': 194, 'mercantile': 195, 'upholstery': 196, 'filtration': 197, 'replenish': 198, 'trajectory': 199, 'peruse': 200, 'barb': 201, 'converge': 202, 'hone': 203, 'angler': 204, 'wildebeest': 205, 'coniferous': 206, 'timpani': 207, 'pilfer': 208, 'pestle': 209, 'repose': 210, 'cupola': 211, 'derrick': 212, 'convex': 213, 'embossed': 214, 'torrent': 215, 'dromedary': 216, 'legume': 217, 'cairn': 218, 'arable': 219, 'supine': 220, 'vitreous': 221, 'lugubrious': 222, 'caster': 223, 'terpsichorean': 224, 'cenotaph': 225, 'calyx': 226, 'osculate': 227, 'tonsorial': 228}\n"
     ]
    }
   ],
   "source": [
    "# only pick words both in graph and rebecca list\n",
    "\n",
    "with open('data/rebecca_words.txt', 'r') as f:\n",
    "    edu_set = []\n",
    "    for word in f:\n",
    "        edu_set.append(word.strip())\n",
    "        \n",
    "rebeccaRank = {w:i+1 for i, w in enumerate(edu_set)}\n",
    "\n",
    "final_words_set = set(token2id.keys()).intersection(edu_set)\n",
    "print(\"# of final words:\\t\", len(final_words_set))\n",
    "print(rebeccaRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word  rebec     strgth        close    betw     eigen  degree  freq\n",
      "0   pick     61  39.275792  1749.764991  1104.0  0.839877     887    50\n",
      "1    eat      9  42.850509  1609.885049  1653.0  0.913919     986   135\n",
      "2   foot      4  42.100066  1426.893151     3.0  0.897943     952    29\n",
      "3  catch     89  38.164909  1684.754610  1868.0  0.817990     852    29\n",
      "4    fly     23  39.287387  1304.106602     0.0  0.840051     885    81\n"
     ]
    }
   ],
   "source": [
    "# create dataframe\n",
    "\n",
    "data = []\n",
    "for i, word in enumerate(token2id.keys()):\n",
    "    word_idx = token2id[word]\n",
    "    if word in final_words_set:\n",
    "        data.append([word,\n",
    "                    rebeccaRank[word],\n",
    "                    strengthRank[i], \n",
    "                    closenessRank[i], \n",
    "                    betweennessRank[i], \n",
    "                    eigen_centralityRank[i],\n",
    "                    degreeRank[i],\n",
    "                    freqRank[i]])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=['word', 'rebec', 'strgth', 'close', 'betw', 'eigen', 'degree','freq'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inspect correlations between Rebecca's rank and our measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using all nodes \n",
    "- for strength and closeness (g)\n",
    "\n",
    "### using max_dist = 0.5 \n",
    "- for betweenness and eigen centrality (g1)\n",
    "\n",
    "### using max_dist = 0.1 \n",
    "- for degree (g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rebecca vs. strgth 0.3135913621782361\n",
      "rebecca vs. close 0.10808335379126607\n",
      "rebecca vs. betw 0.013219015565545184\n",
      "rebecca vs. eigen 0.3295114256127249\n",
      "rebecca vs. degree -0.35072034004458963\n",
      "rebecca vs. freq -0.2575779191436691\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns[2:]:  \n",
    "    print('rebecca vs. {}'.format(col), df['rebec'].corr(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  rebec     strgth        close    betw     eigen  degree  freq\n",
      "35   ball      1  62.704245  1665.439269  4942.0  0.571568    1079    10\n",
      "8     dog      2  95.111616  1519.493029   877.0  0.871343     875    66\n",
      "11  spoon      3  93.671600  1419.554591   225.0  0.846802     883     7\n",
      "2    foot      4  83.597866  1426.893151     3.0  0.780135     952    29\n",
      "7    duck      5  96.382156  1561.317217   174.0  0.903327     877    17\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(by=['rebec']) # using g2-version for everything (highest correlation)\n",
    "print(df.head())\n",
    "df.to_csv(\"./ranks_04_06_continuous_close_strgth.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using max_dist = 0.5 \n",
    "- for strength and closeness, betweenness and eigen centrality (g1)\n",
    "\n",
    "### using max_dist = 0.1 \n",
    "- for degree (g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rebecca vs. strgth 0.32776575680353076\n",
      "rebecca vs. close 0.10808335379126607\n",
      "rebecca vs. betw 0.013219015565545184\n",
      "rebecca vs. eigen 0.3295114256127249\n",
      "rebecca vs. degree -0.35072034004458963\n",
      "rebecca vs. freq -0.2575779191436691\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns[2:]:  \n",
    "    print('rebecca vs. {}'.format(col), df['rebec'].corr(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  rebec     strgth        close    betw     eigen  degree  freq\n",
      "35   ball      1  62.704245  1665.439269  4942.0  0.571568    1079    10\n",
      "8     dog      2  92.688268  1519.493029   877.0  0.871343     875    66\n",
      "11  spoon      3  90.065604  1419.554591   225.0  0.846802     883     7\n",
      "2    foot      4  83.597866  1426.893151     3.0  0.780135     952    29\n",
      "7    duck      5  95.874417  1561.317217   174.0  0.903327     877    17\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(by=['rebec']) # using g2-version for everything (highest correlation)\n",
    "print(df.head())\n",
    "df.to_csv(\"./ranks_04_06_maxdistance_0_5.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using max_dist = 0.1 for everything\n",
    " - strength and closeness, betweenness and eigen centrality, degree (g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rebecca vs. strgth -0.40733964984980287\n",
      "rebecca vs. close 0.10808335379126607\n",
      "rebecca vs. betw 0.013219015565545184\n",
      "rebecca vs. eigen -0.4075250778740258\n",
      "rebecca vs. degree -0.35072034004458963\n",
      "rebecca vs. freq -0.2575779191436691\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns[2:]:  \n",
    "    print('rebecca vs. {}'.format(col), df['rebec'].corr(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  rebec     strgth        close    betw     eigen  degree  freq\n",
      "35   ball      1  44.491207  1665.439269  4942.0  0.947106    1079    10\n",
      "8     dog      2  38.267111  1519.493029   877.0  0.817279     875    66\n",
      "11  spoon      3  38.745796  1419.554591   225.0  0.830752     883     7\n",
      "2    foot      4  42.100066  1426.893151     3.0  0.897943     952    29\n",
      "7    duck      5  39.796608  1561.317217   174.0  0.851659     877    17\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(by=['rebec']) # using g2-version for everything (highest correlation)\n",
    "print(df.head())\n",
    "df.to_csv(\"./ranks_04_06_maxdistance_0_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
