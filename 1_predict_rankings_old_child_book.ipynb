{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import * \n",
    "\n",
    "############## ONLY NEED TO CHANGE THE FOLLOWING TO USE preprocess module##########\n",
    "DIR = 'data' \n",
    "BOOK_FILE = '{}/book.txt'.format(DIR)\n",
    "BOOK_LEMMA_FILE = '{}/lemmatize_book.txt'.format(DIR)\n",
    "PPVT_LEMMA_FILE = '{}/PPVT lemma.csv'.format(DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "4751\n",
      "228\n"
     ]
    }
   ],
   "source": [
    "ranked_words= read_rebecca_lemma(PPVT_LEMMA_FILE)\n",
    "book_words, pos_words = lemmatize_book(BOOK_FILE, BOOK_LEMMA_FILE)\n",
    "generate_wordset_files(book_words, ranked_words, DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "    - Good to see increase in distance for these 3 pairs of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n",
    "from gensim.models import KeyedVectors\n",
    "from igraph import *\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\"\"\"\n",
    "task\n",
    "1. `model.py`: construct simple pipeline to build igraph, form df\n",
    "2. `preprocess.py`: build several graphs (V,N) in ipython book\n",
    "3. `visualize.py`: visualize network by sampling\n",
    "\"\"\"\n",
    "\n",
    "DIR = 'data'\n",
    "# paths to the 3 generated files from preprocess.py\n",
    "BOOK_WORD_SET_FILE = '{}/book_words_set.txt'.format(DIR)\n",
    "REBECCA_WORD_FILE = '{}/rebecca_words.txt'.format(DIR)\n",
    "LEMMA_BOOK_FILE = \"{}/lemmatize_book.txt\".format(DIR)\n",
    "\n",
    "def read_file_to_list(filename):\n",
    "\twords = []\n",
    "\twith open(filename, 'r') as f:\n",
    "\t    for word in f:\n",
    "\t        words.append(word.strip())\n",
    "\treturn words\n",
    "\n",
    "def getSimilarity(w1, w2, model):\n",
    "\n",
    "    return abs(1-spatial.distance.cosine(model.wv[w1], model.wv[w2]))\n",
    "\n",
    "def load_pretrained_word2vec(documents, size, min_count, iters, retrain=False):\n",
    "\tGOOGLE_W2V_FILE = './model/GoogleNews-vectors-negative300.bin'\n",
    "\tmodel = KeyedVectors.load_word2vec_format(GOOGLE_W2V_FILE, binary=True)  \n",
    "\tif not retrain:\n",
    "\t\treturn model\n",
    "\tmodel_2 = gensim.models.Word2Vec(\n",
    "\t            documents,\n",
    "\t            sg=0,\n",
    "\t            size=size,\n",
    "\t            window=20,\n",
    "\t            min_count=min_count,\n",
    "\t            iter=iters,\n",
    "\t            workers=4)\n",
    "\ttotal_examples = model_2.corpus_count\n",
    "\tmodel_2.build_vocab([list(model.vocab.keys())], update=True)\n",
    "\tmodel_2.intersect_word2vec_format(GOOGLE_W2V_FILE, binary=True, lockf=1.0)\n",
    "\tmodel_2.train(documents, total_examples=total_examples, epochs=model_2.epochs)\n",
    "\treturn model_2\n",
    "\n",
    "def train_word2vec(documents, size=300, min_count=5, iters=100, window=20):\n",
    "\tmodel = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        sg=0,\n",
    "        size=size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        iter=iters,\n",
    "        workers=4)\n",
    "\tmodel.train(documents, total_examples=len(documents), epochs=model.epochs)\n",
    "\treturn model\n",
    "\n",
    "def build_graph(model, sampling_size=0):\n",
    "\n",
    "\tbook_set = read_file_to_list(BOOK_WORD_SET_FILE)\n",
    "\tedu_set = read_file_to_list(REBECCA_WORD_FILE)\n",
    "\n",
    "\tprint(\"word2vec:\\t\", len(model.wv.vocab))\n",
    "\tprint(\"book_set:\\t\", len(book_set))\n",
    "\tprint(\"edu_set:\\t\", len(edu_set))\n",
    "# \tsurrounding_words = set(model.wv.vocab.keys()).intersection(book_set)\n",
    "# \tsurrounding_words = set(surrounding_words).difference(edu_set)\n",
    "# \tprint('surrounding words', len(surrounding_words))\n",
    "\tintersection_words = set(model.wv.vocab.keys()).intersection(edu_set)\n",
    "\tprint('intersertion words', len(intersection_words))\n",
    "\n",
    "\tif sampling_size:\n",
    "\t\ttoken2vec = {}\n",
    "\t\trand_words = random.sample(intersection_words, sampling_size)\n",
    "\t\tfor word in rand_words:\n",
    "\t\t\ttoken2vec[word] = model.wv[word]\n",
    "\t\tfor word in intersection_words:\n",
    "\t\t\ttoken2vec[word] = model.wv[word]\n",
    "\telse:\n",
    "\t\ttoken2vec = model.wv.vocab # bad\n",
    "\n",
    "\tidx = 0\n",
    "\tid2token = {}\n",
    "\ttoken2id = {}\n",
    "\n",
    "\tfor word in token2vec:\n",
    "\t    id2token[idx] = word\n",
    "\t    token2id[word] = idx\n",
    "\t    idx += 1\n",
    "\n",
    "\tvertices = [idx for idx in range(len(token2vec))]\n",
    "\tedges = [(i, j) for i in vertices for j in vertices if i < j]\n",
    "\tg = Graph(vertex_attrs={\"label\":vertices}, edges=edges, directed=False)\n",
    "\tg.es[\"sim\"] = [getSimilarity(id2token[i], id2token[j], model) for i,j in edges]\n",
    "\tg.es[\"dist\"] = np.array(1-np.array(g.es['sim'])).tolist()\n",
    "\t# test validity\n",
    "\t# assert(getSimilarity('man', 'woman', model) == g[token2id['man'], token2id['woman']] )\n",
    "\t# assert(getSimilarity('cat', 'dog', model) == g[token2id['cat'], token2id['dog']] )\n",
    "\n",
    "\treturn g, (edges, vertices), (id2token, token2id)\n",
    "\n",
    "def filter_graph(weights, edges, vertices, id2token, min_sim_strength=1.0):\n",
    "\n",
    "\tnew_edges, new_weights = [], [] # weights: similarity\n",
    "\tnew_distances = []\n",
    "\tfor edge, w in zip(edges, weights):\n",
    "\t    if w >= min_sim_strength:\n",
    "\t        new_edges.append(edge)\n",
    "\t        new_weights.append(w)\n",
    "\t        new_distances.append(1-w)\n",
    "\tg = Graph(vertex_attrs={\"label\":vertices}, edges=new_edges, directed=False)\n",
    "\n",
    "\tg.es[\"sim\"] = new_weights\n",
    "\tg.es[\"dist\"] = new_distances\n",
    "\tg.vs[\"label\"] = [id2token[idx] for idx in vertices]\n",
    "\n",
    "\treturn g\n",
    "\n",
    "def compute_measures_df(g, edges, vertices, id2token, token2id):\n",
    "\t\"\"\"\n",
    "\tcompute centrality measures to output df\n",
    "\t\"\"\"\n",
    "\n",
    "\t# g -> strength, closeness (continuous)\n",
    "\tstrengthRank = g.strength(None,  weights=g.es['sim'])\n",
    "\tclosenessRank = g.closeness(None, 'all', weights=g.es['dist'], normalized=True)\n",
    "\n",
    "\t# g1 -> betweenness, eigen_centrality\n",
    "\tg1 = filter_graph(g.es[\"sim\"], edges, vertices, id2token, min_sim_strength=0.5)\n",
    "\n",
    "\tbetweennessRank = g1.betweenness(directed=False, weights=g1.es['dist'])\n",
    "\teigen_centralityRank = g1.eigenvector_centrality(directed=False, weights=g1.es['sim'])\n",
    "\n",
    "\t# g2 -> degree\n",
    "\tg2 = filter_graph(g.es[\"sim\"], edges, vertices, id2token, min_sim_strength=0.5)\n",
    "\n",
    "\tdegreeRank = g2.degree(mode='all')\n",
    "\n",
    "\t# frequency\n",
    "\twordCounter = defaultdict(int)\n",
    "\twith open(LEMMA_BOOK_FILE) as f:\n",
    "\t    for line in f:\n",
    "\t        for w in line.split():\n",
    "\t            w = w.strip()\n",
    "\t            if w in token2id:\n",
    "\t                wordCounter[w] += 1\n",
    "\tfreqRank = [wordCounter[w] for w in token2id]\n",
    "\n",
    "\t# rebecca\n",
    "\twith open(REBECCA_WORD_FILE, 'r') as f:\n",
    "\t    edu_list = []\n",
    "\t    for word in f:\n",
    "\t        edu_list.append(word.strip())\n",
    "\t        \n",
    "\trebeccaRank = {w:i+1 for i, w in enumerate(edu_list)}\n",
    "\tfinal_words_set = set(token2id.keys()).intersection(edu_list)\n",
    "\n",
    "\t# create df\n",
    "\tdata = []\n",
    "\twords_inorder = [id2token[idx] for idx in range(len(token2id))]\n",
    "\tfor i, word in enumerate(words_inorder):\n",
    "\t    if word in final_words_set:\n",
    "\t        data.append([word,\n",
    "\t                    rebeccaRank[word],\n",
    "\t                    strengthRank[i], \n",
    "\t                    closenessRank[i], \n",
    "\t                    betweennessRank[i], \n",
    "\t                    eigen_centralityRank[i],\n",
    "\t                    degreeRank[i],\n",
    "\t                    freqRank[i]])\n",
    "\n",
    "\n",
    "\tdf = pd.DataFrame(data, columns=['word', 'ppvt', 'strgth', 'close', 'betw', 'eigen', 'degree', 'freq'])\n",
    "\tdf = df.sort_values(by=['ppvt'])\n",
    "\tprint(df.head())\n",
    "\treturn df\n",
    "\n",
    "def build_pos_graph(model, pos_word_set, sampling_size=0, max_dist=1):\n",
    "\t\"\"\"\n",
    "\tbuild semantic graph without referring Rebecca's word list;\n",
    "\tonly to explore noun, verb's centrality in vocab\n",
    "\t\"\"\"\n",
    "\n",
    "\tbook_set = read_file_to_list(BOOK_WORD_SET_FILE)\n",
    "\tedu_set = read_file_to_list(REBECCA_WORD_FILE)\n",
    "\tvalid_pos_words = set(model.wv.vocab.keys()).intersection(edu_set)\n",
    "\tvalid_pos_words = set(valid_pos_words).intersection(pos_word_set)\n",
    "\t# print('pos_word_set', len(pos_word_set))\n",
    "\n",
    "\ttoken2vec = {}\n",
    "\tif sampling_size > 0:\n",
    "\t\trand_words = random.sample(valid_pos_words, sampling_size)\n",
    "\t\tfor word in rand_words:\n",
    "\t\t\ttoken2vec[word] = model.wv[word]\t\t\t\n",
    "\telse:\n",
    "\t\ttoken2vec = valid_pos_words\n",
    "\n",
    "\tidx = 0\n",
    "\tid2token = {}\n",
    "\ttoken2id = {}\n",
    "\n",
    "\tfor word in token2vec:\n",
    "\t    id2token[idx] = word\n",
    "\t    token2id[word] = idx\n",
    "\t    idx += 1\n",
    "\n",
    "\tvertices = [idx for idx in range(len(token2vec))]\n",
    "\tedges = [(i, j) for i in vertices for j in vertices if i < j]\n",
    "\tweights = [getSimilarity(id2token[i], id2token[j], model) for i, j in edges]\n",
    "\n",
    "\tg = filter_graph(weights, edges, vertices, id2token, max_dist)\n",
    "\n",
    "\treturn g, (edges, vertices), (id2token, token2id)\n",
    "\n",
    "def read_docs():\n",
    "\twith open(LEMMA_BOOK_FILE) as f:\n",
    "\t    documents = []\n",
    "\t    for line in f:\n",
    "\t        documents.append(line.split())\n",
    "\treturn documents\n",
    "\n",
    "def freq_counts(documents):\n",
    "    allwords = []\n",
    "    for doc in documents:\n",
    "        allwords += doc\n",
    "    \n",
    "    ct = Counter(allwords)\n",
    "    for i in range(5):\n",
    "        print('freq ', i+1, len([w for w in ct if ct[w] >= i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec:\t 10183\n",
      "book_set:\t 4751\n",
      "edu_set:\t 228\n",
      "intersertion words 75\n",
      "     word  ppvt     strgth     close  betw         eigen  degree  freq\n",
      "16   ball     1   6.749831  1.100369   0.0  6.885495e-09       0    10\n",
      "3     dog     2  18.277181  1.356707   4.0  5.739247e-01      14    66\n",
      "70  spoon     3  17.148174  1.337213   2.0  5.864365e-01      13     7\n",
      "35   foot     4  19.469876  1.386460   8.0  8.249493e-01      19    29\n",
      "53   duck     5  19.297002  1.376698   2.0  7.142619e-01      17    17\n",
      "rebecca vs. strgth 0.34670707555383795\n",
      "rebecca vs. close 0.3449171577657438\n",
      "rebecca vs. betw 0.344892356335694\n",
      "rebecca vs. eigen 0.30977558006665695\n",
      "rebecca vs. degree 0.3400108732925529\n",
      "rebecca vs. freq 0.5152850749928397\n"
     ]
    }
   ],
   "source": [
    "# build graph     \n",
    "g, (edges, vertices), (id2token, token2id) = build_graph(model, sampling_size=10)\n",
    "\n",
    "# compute measures\n",
    "df = compute_measures_df(g, edges, vertices, id2token, token2id)\n",
    "\n",
    "# see correlations\n",
    "for col in df.columns[2:]:  \n",
    "    print('rebecca vs. {}'.format(col), df['ppvt'].corr(df[col].rank(ascending=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import *\n",
    "\n",
    "documents = read_docs()\n",
    "model = train_word2vec(documents, size=100, min_count=0, iters=40, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq  1 10183\n",
      "freq  2 2828\n",
      "freq  3 1809\n",
      "freq  4 1454\n",
      "freq  5 1224\n",
      "table girl 0.07762456685304642\n",
      "man woman 0.4199064075946808\n",
      "girl boy 0.5913309454917908\n",
      "sun moon 0.7748672366142273\n",
      "cat dog 0.8432342410087585\n"
     ]
    }
   ],
   "source": [
    "freq_counts(documents)\n",
    "print('table', 'girl', getSimilarity('table', 'girl', model))\n",
    "print('man', 'woman', getSimilarity('man', 'woman', model))\n",
    "print('girl', 'boy', getSimilarity('girl', 'boy', model))\n",
    "print('sun', 'moon', getSimilarity('sun', 'moon', model))\n",
    "print('cat', 'dog', getSimilarity('cat', 'dog', model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmodel = load_pretrained_word2vec(documents, 100, 0, 40, retrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq  1 10183\n",
      "freq  2 2828\n",
      "freq  3 1809\n",
      "freq  4 1454\n",
      "freq  5 1224\n",
      "table girl 0.03306927904486656\n",
      "man woman 0.7664012312889099\n",
      "girl boy 0.8543271422386169\n",
      "sun moon 0.42628341913223267\n",
      "cat dog 0.760945737361908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "freq_counts(documents)\n",
    "print('table', 'girl', getSimilarity('table', 'girl', pmodel))\n",
    "print('man', 'woman', getSimilarity('man', 'woman', pmodel))\n",
    "print('girl', 'boy', getSimilarity('girl', 'boy', pmodel))\n",
    "print('sun', 'moon', getSimilarity('sun', 'moon', pmodel))\n",
    "print('cat', 'dog', getSimilarity('cat', 'dog', pmodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43008167"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "a = model.wv['cat']\n",
    "b = model.wv['dog']\n",
    "cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlapping in Vocab\n",
    "    - therefore, we do not use google pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection b/w Google W2V, FB data, and PPVT is 75\n",
      "Intersection b/w FB data, and PPVT is 85\n"
     ]
    }
   ],
   "source": [
    "vocab_overlapping_googleW2V = len([w for w in ranked_words if w in set(model.wv.vocab)])\n",
    "print(\"Intersection b/w Google W2V, FB data, and PPVT is {}\".format(vocab_overlapping_googleW2V))\n",
    "\n",
    "book_set = set(book_words)\n",
    "vocab_overlapping_fb_data = len([w for w in ranked_words if w in book_set])\n",
    "print(\"Intersection b/w FB data, and PPVT is {}\".format(vocab_overlapping_fb_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "    - nouns\n",
    "    - verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_list(l, lower, upper):\n",
    "\tmax_min_range = max(l)-min(l)\n",
    "# \tprint('times', (upper-lower)/max_min_range)\n",
    "\treturn [lower + (upper - lower) / max_min_range * x for x in l]\n",
    "\n",
    "for i in range(0, 20, 1):\n",
    "\tmax_dist = 0.01*(i+1)\n",
    "\tg, (edges, vertices), (id2token, token2id) = build_pos_graph(model, pos_words['n'], sampling_size=10, max_dist=max_dist)\n",
    "\twordCounter = defaultdict(int)\n",
    "\twith open(LEMMA_BOOK_FILE) as f:\n",
    "\t    for line in f:\n",
    "\t        for w in line.split():\n",
    "\t            w = w.strip()\n",
    "\t            if w in token2id:\n",
    "\t                wordCounter[w] += 1\n",
    "\trank = [wordCounter[w] for w in token2id]\n",
    "\tvertexSize = normalize_list(rank, 10, 40)\n",
    "# \tvertexLabelSize = normalize_list(rank, 20, 40)\n",
    "\tvisual_style = {}\n",
    "\tvisual_style[\"vertex_color\"] = \"#94a1b6\"\n",
    "\tvisual_style[\"vertex_label_color\"] = \"#f20606\"\n",
    "\tvisual_style[\"vertex_label_size\"] = 15 # 15\n",
    "\tvisual_style[\"vertex_size\"] = vertexSize # 15\n",
    "\tvisual_style[\"vertex_label\"] = g.vs[\"label\"]\n",
    "\tlayout = g.layout(\"kk\")\n",
    "\tvisual_style[\"layout\"] = layout\n",
    "\tvisual_style[\"bbox\"] = (300, 300)\n",
    "\tvisual_style[\"margin\"] = 20\n",
    "\tfilename = \"./output/noun{}.pdf\".format(max_dist)\n",
    "\tplot(g, filename, **visual_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_list(l, lower, upper):\n",
    "\tmax_min_range = max(l)-min(l)\n",
    "# \tprint('times', (upper-lower)/max_min_range)\n",
    "\treturn [lower + (upper - lower) / max_min_range * x for x in l]\n",
    "\n",
    "for i in range(0, 20, 1):\n",
    "\tmax_dist = 0.01*(i+1)\n",
    "\tg, (edges, vertices), (id2token, token2id) = build_pos_graph(model, pos_words['v'], sampling_size=10, max_dist=max_dist)\n",
    "\twordCounter = defaultdict(int)\n",
    "\twith open(LEMMA_BOOK_FILE) as f:\n",
    "\t    for line in f:\n",
    "\t        for w in line.split():\n",
    "\t            w = w.strip()\n",
    "\t            if w in token2id:\n",
    "\t                wordCounter[w] += 1\n",
    "\trank = [wordCounter[w] for w in token2id]\n",
    "\tvertexSize = normalize_list(rank, 10, 40)\n",
    "# \tvertexLabelSize = normalize_list(rank, 20, 40)\n",
    "\tvisual_style = {}\n",
    "\tvisual_style[\"vertex_color\"] = \"#94a1b6\"\n",
    "\tvisual_style[\"vertex_label_color\"] = \"#f20606\"\n",
    "\tvisual_style[\"vertex_label_size\"] = 15 # 15\n",
    "\tvisual_style[\"vertex_size\"] = vertexSize # 15\n",
    "\tvisual_style[\"vertex_label\"] = g.vs[\"label\"]\n",
    "\tlayout = g.layout(\"kk\")\n",
    "\tvisual_style[\"layout\"] = layout\n",
    "\tvisual_style[\"bbox\"] = (300, 300)\n",
    "\tvisual_style[\"margin\"] = 20\n",
    "\tfilename = \"./output/noun{}.pdf\".format(max_dist)\n",
    "\tplot(g, filename, **visual_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rank words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec:\t 10183\n",
      "book_set:\t 4751\n",
      "edu_set:\t 228\n",
      "intersertion words 75\n",
      "     word  ppvt     strgth     close  betw     eigen  degree  freq\n",
      "9    ball     1   6.749831  1.100369   0.0  0.049723       0    10\n",
      "52    dog     2  18.277181  1.356707   4.0  0.573873      14    66\n",
      "70  spoon     3  17.148174  1.337213   2.0  0.586251      13     7\n",
      "35   foot     4  19.469876  1.386460   8.0  0.824783      19    29\n",
      "51   duck     5  19.297002  1.376698   2.0  0.714131      17    17\n",
      "rebecca vs. strgth 0.34670707555383795\n",
      "rebecca vs. close 0.3449171577657438\n",
      "rebecca vs. betw 0.344892356335694\n",
      "rebecca vs. eigen 0.30977558006665695\n",
      "rebecca vs. degree 0.3400108732925529\n",
      "rebecca vs. freq 0.5152850749928397\n"
     ]
    }
   ],
   "source": [
    "# from model import *\n",
    "\n",
    "# documents = read_docs()\n",
    "\n",
    "# # no pretrained\n",
    "# model = train_word2vec(documents, 300, 5, 200)\n",
    "# # use pretrained model\n",
    "# # model = load_pretrained_word2vec(documents, 300, 5, 100, retrain=False)\n",
    "# # use pretrained + retrain\n",
    "# # model = load_pretrained_word2vec(documents, 300, 5, 30, retrain=True)\n",
    "\n",
    "# freq_counts(documents)\n",
    "# print('man', 'woman', getSimilarity('man', 'woman', model))\n",
    "# print('cat', 'dog', getSimilarity('cat', 'dog', model))\n",
    "# print('sun', 'kid', getSimilarity('sun', 'kid', model))        \n",
    "        \n",
    "# build graph     \n",
    "g, (edges, vertices), (id2token, token2id) = build_graph(model, sampling_size=10)\n",
    "\n",
    "# compute measures\n",
    "df = compute_measures_df(g, edges, vertices, id2token, token2id)\n",
    "\n",
    "# see correlations\n",
    "for col in df.columns[2:]:  \n",
    "    print('rebecca vs. {}'.format(col), df['ppvt'].corr(df[col].rank(ascending=False)))\n",
    "    \n",
    "# save to csv\n",
    "df.to_csv(\"./output/rankings_0623_old_book.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
