{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import * \n",
    "\n",
    "############## ONLY NEED TO CHANGE THE FOLLOWING TO USE preprocess module##########\n",
    "DIR = 'data' \n",
    "BOOK_FILE = '{}/book.txt'.format(DIR)\n",
    "BOOK_LEMMA_FILE = '{}/lemmatize_book.txt'.format(DIR)\n",
    "PPVT_LEMMA_FILE = '{}/PPVT lemma.csv'.format(DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "4751\n",
      "228\n"
     ]
    }
   ],
   "source": [
    "ranked_words= read_rebecca_lemma(PPVT_LEMMA_FILE)\n",
    "book_words, pos_words = lemmatize_book(BOOK_FILE, BOOK_LEMMA_FILE)\n",
    "generate_wordset_files(book_words, ranked_words, DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "    - Good to see increase in distance for these 3 pairs of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n",
    "from gensim.models import KeyedVectors\n",
    "from igraph import *\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\"\"\"\n",
    "task\n",
    "1. `model.py`: construct simple pipeline to build igraph, form df\n",
    "2. `preprocess.py`: build several graphs (V,N) in ipython book\n",
    "3. `visualize.py`: visualize network by sampling\n",
    "\"\"\"\n",
    "\n",
    "DIR = 'data'\n",
    "# paths to the 3 generated files from preprocess.py\n",
    "BOOK_WORD_SET_FILE = '{}/book_words_set.txt'.format(DIR)\n",
    "REBECCA_WORD_FILE = '{}/rebecca_words.txt'.format(DIR)\n",
    "LEMMA_BOOK_FILE = \"{}/lemmatize_book.txt\".format(DIR)\n",
    "\n",
    "def read_file_to_list(filename):\n",
    "\twords = []\n",
    "\twith open(filename, 'r') as f:\n",
    "\t    for word in f:\n",
    "\t        words.append(word.strip())\n",
    "\treturn words\n",
    "\n",
    "def getSimilarity(w1, w2, model):\n",
    "    return abs(1-spatial.distance.cosine(model.wv[w1], model.wv[w2]))\n",
    "\n",
    "def load_pretrained_word2vec(documents, size, min_count, iters, retrain=False):\n",
    "\tGOOGLE_W2V_FILE = './model/GoogleNews-vectors-negative300.bin'\n",
    "\tmodel = KeyedVectors.load_word2vec_format(GOOGLE_W2V_FILE, binary=True)  \n",
    "\tif not retrain:\n",
    "\t\treturn model\n",
    "\tmodel_2 = gensim.models.Word2Vec(\n",
    "\t            documents,\n",
    "\t            sg=0,\n",
    "\t            size=size,\n",
    "\t            window=20,\n",
    "\t            min_count=min_count,\n",
    "\t            iter=iters,\n",
    "\t            workers=4)\n",
    "\ttotal_examples = model_2.corpus_count\n",
    "\tmodel_2.build_vocab([list(model.vocab.keys())], update=True)\n",
    "\tmodel_2.intersect_word2vec_format(GOOGLE_W2V_FILE, binary=True, lockf=1.0)\n",
    "\tmodel_2.train(documents, total_examples=total_examples, epochs=model_2.epochs)\n",
    "\treturn model_2\n",
    "\n",
    "def train_word2vec(documents, size=300, min_count=5, iters=100, window=20):\n",
    "\tmodel = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        sg=0,\n",
    "        size=size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        iter=iters,\n",
    "        workers=4)\n",
    "\tmodel.train(documents, total_examples=len(documents), epochs=model.epochs)\n",
    "\treturn model\n",
    "\n",
    "def read_docs():\n",
    "\twith open(LEMMA_BOOK_FILE) as f:\n",
    "\t    documents = []\n",
    "\t    for line in f:\n",
    "\t        documents.append(line.split())\n",
    "\treturn documents\n",
    "\n",
    "def freq_counts(documents):\n",
    "    allwords = []\n",
    "    for doc in documents:\n",
    "        allwords += doc\n",
    "    \n",
    "    ct = Counter(allwords)\n",
    "    for i in range(5):\n",
    "        print('freq ', i+1, len([w for w in ct if ct[w] >= i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = read_docs()\n",
    "model = train_word2vec(documents, size=100, min_count=0, iters=40, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq  1 10183\n",
      "freq  2 2828\n",
      "freq  3 1809\n",
      "freq  4 1454\n",
      "freq  5 1224\n",
      "table girl 0.07248084247112274\n",
      "man woman 0.42470675706863403\n",
      "girl boy 0.6075921654701233\n",
      "sun moon 0.7933577299118042\n",
      "cat dog 0.8389257788658142\n"
     ]
    }
   ],
   "source": [
    "freq_counts(documents)\n",
    "print('table', 'girl', getSimilarity('table', 'girl', model))\n",
    "print('man', 'woman', getSimilarity('man', 'woman', model))\n",
    "print('girl', 'boy', getSimilarity('girl', 'boy', model))\n",
    "print('sun', 'moon', getSimilarity('sun', 'moon', model))\n",
    "print('cat', 'dog', getSimilarity('cat', 'dog', model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(model, sampling_size=0):\n",
    "\n",
    "\tbook_set = read_file_to_list(BOOK_WORD_SET_FILE)\n",
    "\tedu_set = read_file_to_list(REBECCA_WORD_FILE)\n",
    "\n",
    "\tprint(\"word2vec:\\t\", len(model.wv.vocab))\n",
    "\tprint(\"book_set:\\t\", len(book_set))\n",
    "\tprint(\"edu_set:\\t\", len(edu_set))\n",
    "# \tsurrounding_words = set(model.wv.vocab.keys()).intersection(book_set)\n",
    "# \tsurrounding_words = set(surrounding_words).difference(edu_set)\n",
    "# \tprint('surrounding words', len(surrounding_words))\n",
    "\tintersection_words = set(model.wv.vocab.keys()).intersection(edu_set)\n",
    "\tprint('intersertion words', len(intersection_words))\n",
    "\n",
    "\tif sampling_size:\n",
    "\t\ttoken2vec = {}\n",
    "\t\trand_words = random.sample(intersection_words, sampling_size)\n",
    "\t\tfor word in rand_words:\n",
    "\t\t\ttoken2vec[word] = model.wv[word]\n",
    "\t\tfor word in intersection_words:\n",
    "\t\t\ttoken2vec[word] = model.wv[word]\n",
    "\telse:\n",
    "\t\ttoken2vec = model.wv.vocab # bad\n",
    "\n",
    "\tidx = 0\n",
    "\tid2token = {}\n",
    "\ttoken2id = {}\n",
    "\n",
    "\tfor word in token2vec:\n",
    "\t    id2token[idx] = word\n",
    "\t    token2id[word] = idx\n",
    "\t    idx += 1\n",
    "\n",
    "\tvertices = [idx for idx in range(len(token2vec))]\n",
    "\tedges = [(i, j) for i in vertices for j in vertices if i < j]\n",
    "\tg = Graph(vertex_attrs={\"label\":vertices}, edges=edges, directed=False)\n",
    "\tg.es[\"sim\"] = [getSimilarity(id2token[i], id2token[j], model) for i,j in edges]\n",
    "\tg.es[\"dist\"] = np.array(1-np.array(g.es['sim'])).tolist()\n",
    "\t# test validity\n",
    "\t# assert(getSimilarity('man', 'woman', model) == g[token2id['man'], token2id['woman']] )\n",
    "\t# assert(getSimilarity('cat', 'dog', model) == g[token2id['cat'], token2id['dog']] )\n",
    "\n",
    "\treturn g, (edges, vertices), (id2token, token2id)\n",
    "\n",
    "def filter_graph(weights, edges, vertices, id2token,  threshold=1.0):\n",
    "\n",
    "\tnew_edges, new_weights = [], [] # weights: similarity\n",
    "\tnew_distances = []\n",
    "\tfor edge, w in zip(edges, weights):\n",
    "\t    if w >= threshold:\n",
    "\t        new_edges.append(edge)\n",
    "\t        new_weights.append(w)\n",
    "\t        new_distances.append(1-w)\n",
    "\tg0 = Graph(vertex_attrs={\"label\":vertices}, edges=new_edges, directed=False)\n",
    "\n",
    "\tg0.es[\"sim\"] = new_weights\n",
    "\tg0.es[\"dist\"] = new_distances\n",
    "\tg0.vs[\"label\"] = [id2token[idx] for idx in vertices]\n",
    "\n",
    "\treturn g0\n",
    "\n",
    "def compute_measures_df(g, edges, vertices, id2token, token2id):\n",
    "\t\"\"\"\n",
    "\tcompute centrality measures to output df\n",
    "\t\"\"\"\n",
    "\n",
    "\t# g -> strength, closeness (continuous)\n",
    "\tstrengthRank = g.strength(None,  weights=g.es['sim'])\n",
    "\tclosenessRank = g.closeness(None, 'all', weights=g.es['dist'], normalized=True)\n",
    "\n",
    "\t# g1 -> betweenness, eigen_centrality\n",
    "\tg1 = filter_graph(g.es[\"sim\"], edges, vertices, id2token, threshold=0.5)\n",
    "    \n",
    "\teigen_centralityRank = g.eigenvector_centrality(directed=False, weights=g.es['sim'])\n",
    "\n",
    "\tbetweennessRank = g.betweenness(vertices=None, \n",
    "                                     directed=False, \n",
    "                                     weights=g.es['dist'])\n",
    "\n",
    "\t# g2 -> degree\n",
    "\tg2 = filter_graph(g.es[\"sim\"], edges, vertices, id2token, threshold=0.2)\n",
    "\n",
    "\tdegreeRank = g2.degree(None, mode='all')\n",
    "\tprint(degreeRank)\n",
    "\n",
    "\t# frequency\n",
    "\twordCounter = defaultdict(int)\n",
    "\twith open(LEMMA_BOOK_FILE) as f:\n",
    "\t    for line in f:\n",
    "\t        for w in line.split():\n",
    "\t            w = w.strip()\n",
    "\t            if w in token2id:\n",
    "\t                wordCounter[w] += 1\n",
    "\tfreqRank = [wordCounter[w] for w in token2id]\n",
    "\n",
    "\t# rebecca\n",
    "\twith open(REBECCA_WORD_FILE, 'r') as f:\n",
    "\t    edu_list = []\n",
    "\t    for word in f:\n",
    "\t        edu_list.append(word.strip())\n",
    "\t        \n",
    "\trebeccaRank = {w:i+1 for i, w in enumerate(edu_list)}\n",
    "\tfinal_words_set = set(token2id.keys()).intersection(edu_list)\n",
    "\n",
    "\tprint(len(strengthRank))\n",
    "\tprint(len(betweennessRank))\n",
    "\tprint(len(closenessRank))\n",
    "\tprint(len(eigen_centralityRank))\n",
    "\tprint(len(degreeRank))\n",
    "\n",
    "\t# create df\n",
    "\tdata = []\n",
    "\twords_inorder = [id2token[idx] for idx in range(len(token2id))]\n",
    "\tfor i, word in enumerate(words_inorder):\n",
    "\t    if word in final_words_set:\n",
    "\t        data.append([word,\n",
    "\t                    rebeccaRank[word],\n",
    "\t                    strengthRank[i], \n",
    "\t                    closenessRank[i], \n",
    "\t                    betweennessRank[i], \n",
    "\t                    eigen_centralityRank[i],\n",
    "\t                    degreeRank[i],\n",
    "\t                    freqRank[i]])\n",
    "\n",
    "\n",
    "\tdf = pd.DataFrame(data, columns=['word', 'ppvt', 'strgth', \n",
    "                                     'close', 'betw', 'eigen', 'degree', 'freq'])\n",
    "\tdf = df.sort_values(by=['ppvt'])\n",
    "\tprint(df.head())\n",
    "\treturn df\n",
    "\n",
    "def build_pos_graph(model, pos_word_set, sampling_size=0, max_dist=1):\n",
    "\t\"\"\"\n",
    "\tbuild semantic graph without referring Rebecca's word list;\n",
    "\tonly to explore noun, verb's centrality in vocab\n",
    "\t\"\"\"\n",
    "\n",
    "\tbook_set = read_file_to_list(BOOK_WORD_SET_FILE)\n",
    "\tedu_set = read_file_to_list(REBECCA_WORD_FILE)\n",
    "\tvalid_pos_words = set(model.wv.vocab.keys()).intersection(edu_set)\n",
    "\tvalid_pos_words = set(valid_pos_words).intersection(pos_word_set)\n",
    "\t# print('pos_word_set', len(pos_word_set))\n",
    "\n",
    "\ttoken2vec = {}\n",
    "\tif sampling_size > 0:\n",
    "\t\trand_words = random.sample(valid_pos_words, sampling_size)\n",
    "\t\tfor word in rand_words:\n",
    "\t\t\ttoken2vec[word] = model.wv[word]\t\t\t\n",
    "\telse:\n",
    "\t\ttoken2vec = valid_pos_words\n",
    "\n",
    "\tidx = 0\n",
    "\tid2token = {}\n",
    "\ttoken2id = {}\n",
    "\n",
    "\tfor word in token2vec:\n",
    "\t    id2token[idx] = word\n",
    "\t    token2id[word] = idx\n",
    "\t    idx += 1\n",
    "\n",
    "\tvertices = [idx for idx in range(len(token2vec))]\n",
    "\tedges = [(i, j) for i in vertices for j in vertices if i < j]\n",
    "\tweights = [getSimilarity(id2token[i], id2token[j], model) for i, j in edges]\n",
    "\n",
    "\tg = filter_graph(weights, edges, vertices, id2token, max_dist)\n",
    "\n",
    "\treturn g, (edges, vertices), (id2token, token2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec:\t 10183\n",
      "book_set:\t 4751\n",
      "edu_set:\t 228\n",
      "intersertion words 75\n",
      "[4, 7, 3, 1, 4, 30, 9, 2, 34, 3, 2, 1, 9, 31, 26, 33, 31, 14, 5, 1, 11, 3, 0, 31, 5, 2, 13, 3, 29, 30, 29, 33, 34, 1, 12, 21, 14, 26, 6, 24, 3, 26, 28, 0, 1, 30, 33, 3, 32, 27, 30, 30, 3, 31, 0, 9, 6, 2, 1, 2, 26, 2, 2, 31, 2, 4, 15, 4, 33, 22, 7, 30, 12, 3, 1]\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "     word  ppvt     strgth     close  betw     eigen  degree  freq\n",
      "67   ball     1   5.404695  1.078791   0.0  0.133588       4    10\n",
      "71    dog     2  18.106690  1.350406   0.0  0.759688      30    66\n",
      "49  spoon     3  18.023623  1.358179   6.0  0.749242      27     7\n",
      "42   foot     4  20.275800  1.405886  14.0  0.861448      28    29\n",
      "15   duck     5  19.830091  1.388278   1.0  0.819107      33    17\n",
      "rebecca vs. strgth 0.3474588410248375\n",
      "rebecca vs. close 0.348831111329043\n",
      "rebecca vs. betw 0.2850541128777261\n",
      "rebecca vs. eigen 0.3743792045577737\n",
      "rebecca vs. degree 0.35423651603868406\n",
      "rebecca vs. freq 0.5152850749928397\n"
     ]
    }
   ],
   "source": [
    "# build graph     \n",
    "g, (edges, vertices), (id2token, token2id) = build_graph(model, sampling_size=30)\n",
    "\n",
    "# compute measures\n",
    "df = compute_measures_df(g, edges, vertices, id2token, token2id)\n",
    "\n",
    "# see correlations\n",
    "for col in df.columns[2:]:  \n",
    "    print('rebecca vs. {}'.format(col), df['ppvt'].corr(df[col].rank(ascending=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mname in ['strgth', 'close', 'betw', 'eigen', 'degree', 'freq']:\n",
    "    cname = mname + '-rank'\n",
    "    df[cname] = df[mname].rank(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ppvt</th>\n",
       "      <th>strgth</th>\n",
       "      <th>close</th>\n",
       "      <th>betw</th>\n",
       "      <th>eigen</th>\n",
       "      <th>degree</th>\n",
       "      <th>freq</th>\n",
       "      <th>strgth-rank</th>\n",
       "      <th>close-rank</th>\n",
       "      <th>betw-rank</th>\n",
       "      <th>eigen-rank</th>\n",
       "      <th>degree-rank</th>\n",
       "      <th>freq-rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>ball</td>\n",
       "      <td>1</td>\n",
       "      <td>5.404695</td>\n",
       "      <td>1.078791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133588</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>62.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>dog</td>\n",
       "      <td>2</td>\n",
       "      <td>18.106690</td>\n",
       "      <td>1.350406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.759688</td>\n",
       "      <td>19</td>\n",
       "      <td>66</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>spoon</td>\n",
       "      <td>3</td>\n",
       "      <td>18.023623</td>\n",
       "      <td>1.358179</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.749242</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>foot</td>\n",
       "      <td>4</td>\n",
       "      <td>20.275800</td>\n",
       "      <td>1.405886</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.861448</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>duck</td>\n",
       "      <td>5</td>\n",
       "      <td>19.830091</td>\n",
       "      <td>1.388278</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.819107</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  ppvt     strgth     close  betw     eigen  degree  freq  \\\n",
       "69   ball     1   5.404695  1.078791   0.0  0.133588       0    10   \n",
       "73    dog     2  18.106690  1.350406   0.0  0.759688      19    66   \n",
       "53  spoon     3  18.023623  1.358179   6.0  0.749242      15     7   \n",
       "45   foot     4  20.275800  1.405886  14.0  0.861448      19    29   \n",
       "58   duck     5  19.830091  1.388278   1.0  0.819107      20    17   \n",
       "\n",
       "    strgth-rank  close-rank  betw-rank  eigen-rank  degree-rank  freq-rank  \n",
       "69         62.0        62.0       48.0        59.0         54.0       17.5  \n",
       "73         18.0        19.0       48.0        16.0         15.0        2.0  \n",
       "53         19.0        17.0       11.0        19.0         21.0       23.5  \n",
       "45         11.0        11.0        8.0        10.0         15.0        5.0  \n",
       "58         12.0        13.0       19.5        12.0         12.0        9.0  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmodel = load_pretrained_word2vec(documents, 100, 0, 40, retrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq  1 10183\n",
      "freq  2 2828\n",
      "freq  3 1809\n",
      "freq  4 1454\n",
      "freq  5 1224\n",
      "table girl 0.03306927904486656\n",
      "man woman 0.7664012312889099\n",
      "girl boy 0.8543271422386169\n",
      "sun moon 0.42628341913223267\n",
      "cat dog 0.760945737361908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "freq_counts(documents)\n",
    "print('table', 'girl', getSimilarity('table', 'girl', pmodel))\n",
    "print('man', 'woman', getSimilarity('man', 'woman', pmodel))\n",
    "print('girl', 'boy', getSimilarity('girl', 'boy', pmodel))\n",
    "print('sun', 'moon', getSimilarity('sun', 'moon', pmodel))\n",
    "print('cat', 'dog', getSimilarity('cat', 'dog', pmodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43008167"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "a = model.wv['cat']\n",
    "b = model.wv['dog']\n",
    "cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlapping in Vocab\n",
    "    - therefore, we do not use google pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection b/w Google W2V, FB data, and PPVT is 75\n",
      "Intersection b/w FB data, and PPVT is 85\n"
     ]
    }
   ],
   "source": [
    "vocab_overlapping_googleW2V = len([w for w in ranked_words if w in set(model.wv.vocab)])\n",
    "print(\"Intersection b/w Google W2V, FB data, and PPVT is {}\".format(vocab_overlapping_googleW2V))\n",
    "\n",
    "book_set = set(book_words)\n",
    "vocab_overlapping_fb_data = len([w for w in ranked_words if w in book_set])\n",
    "print(\"Intersection b/w FB data, and PPVT is {}\".format(vocab_overlapping_fb_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "    - nouns\n",
    "    - verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_list(l, lower, upper):\n",
    "\tmax_min_range = max(l)-min(l)\n",
    "# \tprint('times', (upper-lower)/max_min_range)\n",
    "\treturn [lower + (upper - lower) / max_min_range * x for x in l]\n",
    "\n",
    "for i in range(0, 20, 1):\n",
    "\tmax_dist = 0.01*(i+1)\n",
    "\tg, (edges, vertices), (id2token, token2id) = build_pos_graph(model, pos_words['n'], sampling_size=10, max_dist=max_dist)\n",
    "\twordCounter = defaultdict(int)\n",
    "\twith open(LEMMA_BOOK_FILE) as f:\n",
    "\t    for line in f:\n",
    "\t        for w in line.split():\n",
    "\t            w = w.strip()\n",
    "\t            if w in token2id:\n",
    "\t                wordCounter[w] += 1\n",
    "\trank = [wordCounter[w] for w in token2id]\n",
    "\tvertexSize = normalize_list(rank, 10, 40)\n",
    "# \tvertexLabelSize = normalize_list(rank, 20, 40)\n",
    "\tvisual_style = {}\n",
    "\tvisual_style[\"vertex_color\"] = \"#94a1b6\"\n",
    "\tvisual_style[\"vertex_label_color\"] = \"#f20606\"\n",
    "\tvisual_style[\"vertex_label_size\"] = 15 # 15\n",
    "\tvisual_style[\"vertex_size\"] = vertexSize # 15\n",
    "\tvisual_style[\"vertex_label\"] = g.vs[\"label\"]\n",
    "\tlayout = g.layout(\"kk\")\n",
    "\tvisual_style[\"layout\"] = layout\n",
    "\tvisual_style[\"bbox\"] = (300, 300)\n",
    "\tvisual_style[\"margin\"] = 20\n",
    "\tfilename = \"./output/noun{}.pdf\".format(max_dist)\n",
    "\tplot(g, filename, **visual_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_list(l, lower, upper):\n",
    "\tmax_min_range = max(l)-min(l)\n",
    "# \tprint('times', (upper-lower)/max_min_range)\n",
    "\treturn [lower + (upper - lower) / max_min_range * x for x in l]\n",
    "\n",
    "for i in range(0, 20, 1):\n",
    "\tmax_dist = 0.01*(i+1)\n",
    "\tg, (edges, vertices), (id2token, token2id) = build_pos_graph(model, pos_words['v'], sampling_size=10, max_dist=max_dist)\n",
    "\twordCounter = defaultdict(int)\n",
    "\twith open(LEMMA_BOOK_FILE) as f:\n",
    "\t    for line in f:\n",
    "\t        for w in line.split():\n",
    "\t            w = w.strip()\n",
    "\t            if w in token2id:\n",
    "\t                wordCounter[w] += 1\n",
    "\trank = [wordCounter[w] for w in token2id]\n",
    "\tvertexSize = normalize_list(rank, 10, 40)\n",
    "# \tvertexLabelSize = normalize_list(rank, 20, 40)\n",
    "\tvisual_style = {}\n",
    "\tvisual_style[\"vertex_color\"] = \"#94a1b6\"\n",
    "\tvisual_style[\"vertex_label_color\"] = \"#f20606\"\n",
    "\tvisual_style[\"vertex_label_size\"] = 15 # 15\n",
    "\tvisual_style[\"vertex_size\"] = vertexSize # 15\n",
    "\tvisual_style[\"vertex_label\"] = g.vs[\"label\"]\n",
    "\tlayout = g.layout(\"kk\")\n",
    "\tvisual_style[\"layout\"] = layout\n",
    "\tvisual_style[\"bbox\"] = (300, 300)\n",
    "\tvisual_style[\"margin\"] = 20\n",
    "\tfilename = \"./output/noun{}.pdf\".format(max_dist)\n",
    "\tplot(g, filename, **visual_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rank words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec:\t 10183\n",
      "book_set:\t 4751\n",
      "edu_set:\t 228\n",
      "intersertion words 75\n",
      "[2, 30, 22, 2, 3, 14, 3, 33, 3, 30, 29, 30, 34, 9, 1, 12, 29, 5, 21, 14, 26, 6, 4, 24, 3, 3, 31, 26, 26, 34, 2, 28, 0, 0, 1, 33, 3, 32, 9, 27, 30, 30, 3, 13, 33, 31, 0, 9, 31, 6, 31, 1, 2, 4, 1, 1, 26, 2, 11, 2, 31, 2, 1, 4, 15, 4, 33, 5, 7, 7, 30, 12, 3, 1, 2]\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "     word  ppvt     strgth     close  betw     eigen  degree  freq\n",
      "65   ball     1   5.404695  1.078791   0.0  0.133588       4    10\n",
      "70    dog     2  18.106690  1.350406   0.0  0.759688      30    66\n",
      "39  spoon     3  18.023623  1.358179   6.0  0.749242      27     7\n",
      "31   foot     4  20.275800  1.405886  14.0  0.861448      28    29\n",
      "44   duck     5  19.830091  1.388278   1.0  0.819107      33    17\n",
      "rebecca vs. strgth 0.3474588410248375\n",
      "rebecca vs. close 0.348831111329043\n",
      "rebecca vs. betw 0.2850541128777261\n",
      "rebecca vs. eigen 0.3743792045577737\n",
      "rebecca vs. degree 0.35423651603868406\n",
      "rebecca vs. freq 0.5152850749928397\n"
     ]
    }
   ],
   "source": [
    "# from model import *\n",
    "\n",
    "# documents = read_docs()\n",
    "\n",
    "# # no pretrained\n",
    "# model = train_word2vec(documents, 300, 5, 200)\n",
    "# # use pretrained model\n",
    "# # model = load_pretrained_word2vec(documents, 300, 5, 100, retrain=False)\n",
    "# # use pretrained + retrain\n",
    "# # model = load_pretrained_word2vec(documents, 300, 5, 30, retrain=True)\n",
    "\n",
    "# freq_counts(documents)\n",
    "# print('man', 'woman', getSimilarity('man', 'woman', model))\n",
    "# print('cat', 'dog', getSimilarity('cat', 'dog', model))\n",
    "# print('sun', 'kid', getSimilarity('sun', 'kid', model))        \n",
    "        \n",
    "# build graph     \n",
    "g, (edges, vertices), (id2token, token2id) = build_graph(model, sampling_size=10)\n",
    "\n",
    "# compute measures\n",
    "df = compute_measures_df(g, edges, vertices, id2token, token2id)\n",
    "\n",
    "# see correlations\n",
    "for col in df.columns[2:]:  \n",
    "    print('rebecca vs. {}'.format(col), df['ppvt'].corr(df[col].rank(ascending=False)))\n",
    "    \n",
    "for mname in ['strgth', 'close', 'betw', 'eigen', 'degree', 'freq']:\n",
    "    cname = mname + '-rank'\n",
    "    df[cname] = df[mname].rank(ascending=False)\n",
    "\n",
    "# save to csv\n",
    "df.to_csv(\"./output/rankings_0625_old_book.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
