{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a few things to do\n",
    "- process the word list\n",
    "- create the igraph\n",
    "- generate rank dataframe for Abdell to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construct word2vec from book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a9d661459412>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/lemma_book.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdocuments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "with open('./data/lemma_book.txt') as f:\n",
    "    documents = []\n",
    "    for line in f:\n",
    "        documents.append(line.split())\n",
    "# print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize and validate the word embeddings\n",
    "    - tweaking iter, epoch improves quality of word embeddings\n",
    "    - use iter=500, epoch=20, for now\n",
    "    - trained only on book data (no external/pre-trained knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=1224, size=300, alpha=0.025)\n",
      "------ 20 iters ----------\n",
      "0.09285947680473328\n",
      "0.9575470089912415\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "model = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        sg=0,\n",
    "        size=300,\n",
    "        window=20,\n",
    "        min_count=5,\n",
    "        iter=20, # this is important, b/c we want overfitting\n",
    "        workers=4)\n",
    "model.train(documents, total_examples=len(documents), epochs=20)\n",
    "print(model)\n",
    "\n",
    "def getDistance(w1, w2):\n",
    "    return abs(1-spatial.distance.cosine(model.wv[w1], model.wv[w2]))\n",
    "\n",
    "print('------ 20 iters ----------')\n",
    "print(getDistance('man', 'woman'))\n",
    "print(getDistance('cat', 'dog'))\n",
    "# print(getDistance('king', 'queen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=2828, size=300, alpha=0.025)\n",
      "------ 100 iters ----------\n",
      "0.10086068511009216\n",
      "0.6262087225914001\n",
      "0.0211331807076931\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "model = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        sg=0,\n",
    "        size=300,\n",
    "        window=20,\n",
    "        min_count=2,\n",
    "        iter=100, # this is important, b/c we want overfitting\n",
    "        workers=4)\n",
    "model.train(documents, total_examples=len(documents), epochs=20)\n",
    "print(model)\n",
    "\n",
    "def getDistance(w1, w2):\n",
    "    return abs(1-spatial.distance.cosine(model.wv[w1], model.wv[w2]))\n",
    "\n",
    "print('------ 100 iters ----------')\n",
    "print(getDistance('man', 'woman'))\n",
    "print(getDistance('cat', 'dog'))\n",
    "print(getDistance('king', 'queen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=2828, size=300, alpha=0.025)\n",
      "------ 300 iters ----------\n",
      "0.17458155751228333\n",
      "0.447554349899292\n",
      "0.02189727872610092\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "model = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        sg=0,\n",
    "        size=300,\n",
    "        window=20,\n",
    "        min_count=2,\n",
    "        iter=300, # this is important, b/c we want overfitting\n",
    "        workers=4)\n",
    "model.train(documents, total_examples=len(documents), epochs=20)\n",
    "print(model)\n",
    "\n",
    "def getDistance(w1, w2):\n",
    "    return abs(1-spatial.distance.cosine(model.wv[w1], model.wv[w2]))\n",
    "\n",
    "print('------ 300 iters ----------')\n",
    "print(getDistance('man', 'woman'))\n",
    "print(getDistance('cat', 'dog'))\n",
    "print(getDistance('king', 'queen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=1224, size=300, alpha=0.025)\n",
      "------ 500 iters ----------\n",
      "0.060461245477199554\n",
      "0.37299448251724243\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "model = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        sg=0,\n",
    "        size=300,\n",
    "        window=20,\n",
    "        min_count=5,\n",
    "        iter=500, # this is important, b/c we want overfitting\n",
    "        workers=4)\n",
    "model.train(documents, total_examples=len(documents), epochs=20)\n",
    "print(model)\n",
    "\n",
    "def getDistance(w1, w2):\n",
    "    return abs(1-spatial.distance.cosine(model.wv[w1], model.wv[w2]))\n",
    "\n",
    "print('------ 500 iters ----------')\n",
    "print(getDistance('man', 'woman'))\n",
    "print(getDistance('cat', 'dog'))\n",
    "# print(getDistance('king', 'queen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save & load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "path = get_tmpfile(\"word2vec.model\")\n",
    "model.save(\"word2vec.model\")\n",
    "model = gensim.models.Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build igraph using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec:\t 1224\n",
      "book_set:\t 4751\n",
      "edu_set:\t 228\n"
     ]
    }
   ],
   "source": [
    "# TODO: examine the word2vec effectiveness using visualizations\n",
    "\n",
    "# with open('data/intersection.txt', 'r') as f:\n",
    "#     inter_set = []\n",
    "#     for word in f:\n",
    "#         inter_set.append(word)\n",
    "\n",
    "with open('data/book_words_set.txt', 'r') as f:\n",
    "    book_set = []\n",
    "    for word in f:\n",
    "        book_set.append(word)\n",
    "\n",
    "with open('data/rebecca_words.txt', 'r') as f:\n",
    "    edu_set = []\n",
    "    for word in f:\n",
    "        edu_set.append(word)\n",
    "        \n",
    "print(\"word2vec:\\t\", len(model.wv.vocab))\n",
    "# print(\"inter_set:\\t\", len(inter_set))\n",
    "print(\"book_set:\\t\", len(book_set))\n",
    "print(\"edu_set:\\t\", len(edu_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct igraph based on word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from igraph import *\n",
    "# token2vec\n",
    "token2vec = model.wv.vocab\n",
    "# create id2token\n",
    "idx = 0\n",
    "id2token = {}\n",
    "token2id = {}\n",
    "words \n",
    "for word in token2vec:\n",
    "    id2token[idx] = word\n",
    "    token2id[word] = idx\n",
    "    idx += 1\n",
    "\n",
    "vertices = [idx for idx in range(len(token2vec))]\n",
    "edges = [(i, j) for i in vertices for j in vertices if i < j]\n",
    "g = Graph(vertex_attrs={\"label\":vertices}, edges=edges, directed=False)\n",
    "    \n",
    "g.vs[\"word\"] = words.keys()\n",
    "weights = [getDistance(id2token[i], id2token[j]) for i, j in edges]\n",
    "g.es[\"weight\"] = weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the validity of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(getDistance('man', 'woman') == g[token2id['man'], token2id['woman']] )\n",
    "assert(getDistance('cat', 'dog') == g[token2id['cat'], token2id['dog']] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality Measures of graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "strengthRank = g2.strength(None,  weights=g2.es['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37.450281295969035,\n",
       " 41.4577521471947,\n",
       " 38.27075478559709,\n",
       " 42.38103917868648,\n",
       " 37.95091226203294,\n",
       " 41.44975711300867,\n",
       " 42.29899334170841,\n",
       " 41.77512261062293,\n",
       " 44.814972953652614,\n",
       " 38.73943052738855]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strengthRank[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "closenessRank = g2.closeness(None, 'all', weights=g2.es['weight'], normalized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1640.1718075756514,\n",
       " 1560.6994895158048,\n",
       " 1372.73361439548,\n",
       " 1725.3999198642887,\n",
       " 1793.039373291286,\n",
       " 1999.8577433068444,\n",
       " 1993.6150039523927,\n",
       " 1817.171252494204,\n",
       " 2091.06016232289,\n",
       " 1765.687344501397]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closenessRank[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### betweenness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove edges whose dsitance >= 0.5\n",
    "max_dist=0.5\n",
    "\n",
    "new_edges, new_weights = [], []\n",
    "for edge, dist in zip(edges, weights):\n",
    "    if dist < max_dist:\n",
    "        new_edges.append(edge)\n",
    "        new_weights.append(dist)\n",
    "g1 = Graph(vertex_attrs={\"label\":vertices}, edges=new_edges, directed=False)\n",
    "g1.vs[\"word\"] = words.keys()\n",
    "g1.es[\"weight\"] = new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "betweennessRank = g2.betweenness(directed=False, weights=g2.es['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2182.0, 121.0, 0.0, 2092.0, 3435.0, 8678.0, 11481.0, 7251.0, 18909.0, 2885.0]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betweennessRank[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eigen_centrality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_centralityRank = g2.eigenvector_centrality(directed=False, weights=g2.es['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8059955392207956,\n",
       " 0.8916044740535753,\n",
       " 0.8241751917866988,\n",
       " 0.9085864311307865,\n",
       " 0.8196731211567353,\n",
       " 0.8868333184381991,\n",
       " 0.9069631463712312,\n",
       " 0.8960718432158904,\n",
       " 0.95833428996227,\n",
       " 0.8355183596354967]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigen_centralityRank[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove edges\n",
    "max_dist=0.1\n",
    "\n",
    "new_edges, new_weights = [], []\n",
    "for edge, dist in zip(edges, weights):\n",
    "    if dist < max_dist:\n",
    "        new_edges.append(edge)\n",
    "        new_weights.append(dist)\n",
    "g2 = Graph(vertex_attrs={\"label\":vertices}, edges=new_edges, directed=False)\n",
    "g2.vs[\"word\"] = words.keys()\n",
    "g2.es[\"weight\"] = new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "degreeRank = g2.degree(mode='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[863, 955, 864, 1034, 868, 1012, 966, 1062, 1098, 869]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degreeRank[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 147, 9, 419, 67, 686, 109, 1466, 3269, 26]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem with frequency (only counts words in the graph)\n",
    "from collections import defaultdict\n",
    "wordCounter = defaultdict(int)\n",
    "with open(\"data/lemma_book.txt\") as f:\n",
    "    for line in f:\n",
    "        for w in line.split():\n",
    "            w = w.strip()\n",
    "            if w in token2id:\n",
    "                wordCounter[w] += 1\n",
    "freqRank = [wordCounter[w] for w in token2id]\n",
    "freqRank[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate dataframe for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of final words:\t 39\n",
      "{'ball': 1, 'dog': 2, 'spoon': 3, 'foot': 4, 'duck': 5, 'banana': 6, 'shoe': 7, 'cup': 8, 'eat': 9, 'bus': 10, 'flower': 11, 'mouth': 12, 'pencil': 13, 'cookie': 14, 'drum': 15, 'turtle': 16, 'red': 17, 'jump': 18, 'carrot': 19, 'read': 20, 'toe': 21, 'belt': 22, 'fly': 23, 'paint': 24, 'dance': 25, 'whistle': 26, 'kick': 27, 'lamp': 28, 'square': 29, 'fence': 30, 'empty': 31, 'happy': 32, 'fire': 33, 'castle': 34, 'squirrel': 35, 'throw': 36, 'farm': 37, 'penguin': 38, 'gift': 39, 'feather': 40, 'cobweb': 41, 'elbow': 42, 'juggle': 43, 'fountain': 44, 'net': 45, 'shoulder': 46, 'dress': 47, 'roof': 48, 'peek': 49, 'ruler': 50, 'tunnel': 51, 'branch': 52, 'envelope': 53, 'diamond': 54, 'calender': 55, 'buckle': 56, 'sawing': 57, 'panda': 58, 'vest': 59, 'arrow': 60, 'pick': 61, 'target': 62, 'drip': 63, 'knight': 64, 'deliver': 65, 'cactus': 66, 'dentist': 67, 'float': 68, 'claw': 69, 'uniform': 70, 'gigantic': 71, 'furry': 72, 'violin': 73, 'group': 74, 'globe': 75, 'vehicle': 76, 'chef': 77, 'squash': 78, 'ax': 79, 'flamingo': 80, 'chimney': 81, 'sort': 82, 'waist': 83, 'vegetable': 84, 'hyena': 85, 'plumber': 86, 'river': 87, 'timer': 88, 'catch': 89, 'trunk': 90, 'vase': 91, 'harp': 92, 'bloom': 93, 'horrified': 94, 'swamp': 95, 'heart': 96, 'pigeon': 97, 'ankle': 98, 'flaming': 99, 'wrench': 100, 'aquarium': 101, 'refuel': 102, 'safe': 103, 'boulder': 104, 'reptile': 105, 'canoe': 106, 'athlete': 107, 'towing': 108, 'luggage': 109, 'direct': 110, 'vine': 111, 'digital': 112, 'dissect': 113, 'predatory': 114, 'hydrant': 115, 'surprised': 116, 'palm': 117, 'clarinet': 118, 'valley': 119, 'kiwi': 120, 'interview': 121, 'pastry': 122, 'assist': 123, 'fragile': 124, 'solo': 125, 'snarl': 126, 'puzzled': 127, 'beverage': 128, 'inflated': 129, 'tusk': 130, 'trumpet': 131, 'rodent': 132, 'inhale': 133, 'links': 134, 'pollute': 135, 'archaeologist': 136, 'coast': 137, 'inject': 138, 'fern': 139, 'mammal': 140, 'demolish': 141, 'isolation': 142, 'clamp': 143, 'dilapidated': 144, 'pedestrian': 145, 'interior': 146, 'garment': 147, 'depart': 148, 'feline': 149, 'hedge': 150, 'citrus': 151, 'florist': 152, 'hover': 153, 'aquatic': 154, 'reprimand': 155, 'carpenter': 156, 'primate': 157, 'glider': 158, 'weary': 159, 'hatchet': 160, 'transparent': 161, 'sedan': 162, 'constrained': 163, 'valve': 164, 'parallelogram': 165, 'pillar': 166, 'consume': 167, 'currency': 168, 'hazardous': 169, 'pentagon': 170, 'appliance': 171, 'poultry': 172, 'cornea': 173, 'peninsula': 174, 'porcelain': 175, 'detonation': 176, 'cerebral': 177, 'perpendicular': 178, 'submerge': 179, 'syringe': 180, 'lever': 181, 'apparel': 182, 'talon': 183, 'cultivate': 184, 'wedge': 185, 'ascend': 186, 'depleted': 187, 'sternum': 188, 'maritime': 189, 'incarcerate': 190, 'dejected': 191, 'quintet': 192, 'incandescent': 193, 'confide': 194, 'mercantile': 195, 'upholstery': 196, 'filtration': 197, 'replenish': 198, 'trajectory': 199, 'peruse': 200, 'barb': 201, 'converge': 202, 'hone': 203, 'angler': 204, 'wildebeest': 205, 'coniferous': 206, 'timpani': 207, 'pilfer': 208, 'pestle': 209, 'repose': 210, 'cupola': 211, 'derrick': 212, 'convex': 213, 'embossed': 214, 'torrent': 215, 'dromedary': 216, 'legume': 217, 'cairn': 218, 'arable': 219, 'supine': 220, 'vitreous': 221, 'lugubrious': 222, 'caster': 223, 'terpsichorean': 224, 'cenotaph': 225, 'calyx': 226, 'osculate': 227, 'tonsorial': 228}\n"
     ]
    }
   ],
   "source": [
    "# only pick words both in graph and rebecca list\n",
    "\n",
    "with open('data/rebecca_words.txt', 'r') as f:\n",
    "    edu_set = []\n",
    "    for word in f:\n",
    "        edu_set.append(word.strip())\n",
    "        \n",
    "rebeccaRank = {w:i+1 for i, w in enumerate(edu_set)}\n",
    "\n",
    "final_words_set = set(token2id.keys()).intersection(edu_set)\n",
    "print(\"# of final words:\\t\", len(final_words_set))\n",
    "print(rebeccaRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word  rebec     strgth        close     betw     eigen  degree  freq\n",
      "0   pick     61  39.271378  1552.453369    778.0  0.845262     881    50\n",
      "1    eat      9  42.516464  2001.123908  16473.0  0.911681     989   135\n",
      "2   foot      4  41.647457  1138.184542     31.0  0.892675     950    29\n",
      "3  catch     89  38.877337  1473.191100    456.0  0.838180     854    29\n",
      "4    fly     23  40.017928  1833.844072   6132.0  0.861678     900    81\n"
     ]
    }
   ],
   "source": [
    "# create dataframe\n",
    "\n",
    "data = []\n",
    "for i, word in enumerate(token2id.keys()):\n",
    "    word_idx = token2id[word]\n",
    "    if word in final_words_set:\n",
    "        data.append([word,\n",
    "                    rebeccaRank[word],\n",
    "                    strengthRank[i], \n",
    "                    closenessRank[i], \n",
    "                    betweennessRank[i], \n",
    "                    eigen_centralityRank[i],\n",
    "                    degreeRank[i],\n",
    "                    freqRank[i]])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=['word', 'rebec', 'strgth', 'close', 'betw', 'eigen', 'degree','freq'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inspect correlations between Rebecca's rank and our measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using all nodes for strength and closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rebecca vs. strgth 0.3339430480684464\n",
      "rebecca vs. close -0.1836991936367226\n",
      "rebecca vs. betw -0.3387505231017212\n",
      "rebecca vs. eigen 0.3429530589923347\n",
      "rebecca vs. degree -0.364914025988664\n",
      "rebecca vs. freq -0.2575779191436691\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns[2:]:  \n",
    "    print('rebecca vs. {}'.format(col), df['rebec'].corr(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using g1 for strength and closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rebecca vs. strgth 0.34766129701642823\n",
      "rebecca vs. close -0.1836991936367226\n",
      "rebecca vs. betw -0.3387505231017212\n",
      "rebecca vs. eigen 0.3429530589923347\n",
      "rebecca vs. degree -0.364914025988664\n",
      "rebecca vs. freq -0.2575779191436691\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns[2:]:  \n",
    "    print('rebecca vs. {}'.format(col), df['rebec'].corr(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using g2 for everything (reason: degree shows strong correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rebecca vs. strgth -0.36432207290381824\n",
      "rebecca vs. close -0.1836991936367226\n",
      "rebecca vs. betw -0.3387505231017212\n",
      "rebecca vs. eigen -0.3536867425684928\n",
      "rebecca vs. degree -0.364914025988664\n",
      "rebecca vs. freq -0.2575779191436691\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns[2:]:  \n",
    "    print('rebecca vs. {}'.format(col), df['rebec'].corr(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['rebec']) # using g2-version for everything (highest correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./ranks_04_04.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
